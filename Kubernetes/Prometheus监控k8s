

面临问题：阿里云的k8s环境和本地服务器的k8s环境的监控问题，需要实现资源的消耗、pods的状态等等

早期版本的k8s直接用prometheus去读取就行，随着版本的更新，现在读取k8s的数据是需要授权访问的


实现方式：外部跑一个grafana读取资源外放的端口，k8s内部跑一个prometheus抓取数据给外面的grafana读。

要实现对 Kubernetes 集群的监控，因为 Kubernetes 的 rbac 机制以及证书认证，当然是把 Prometheus 部署在 Kubernetes 集群上最方便。可是我们目前的监控系统是以 Kubernetes 集群外部的 Prometheus 为主的，Grafana 和告警都是使用这个外部的 Prometheus，如果还需要在 Kubernetes 集群内部部署一个 Prometheus 的话一定要把它桶外部的 Prometheus 联合起来，好在 Prometheus 支持 Federation。

Federation 允许一个 Prometheus 从另一个 Prometheus 中拉取某些指定的时序数据。Federation 是 Prometheus 提供的扩展机制，允许 Prometheus 从一个节点扩展到多个节点，实际使用中一般会扩展成树状的层级结构。

在 Kubernetes 上部署 Prometheus 十分简单，只需要下面 4 个文件：prometheus.rbac.yml, prometheus.config.yml, prometheus.deploy.yml, prometheus.svc.yml。 下面给的例子中将 Prometheus 部署到 kube-system 命名空间。

1、prometheus.rbac.yml 定义了 Prometheus 容器访问 Kubernetes apiserver所需的 ServiceAccount 和 ClusterRole 及ClusterRoleBinding

[source,bash]
----
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics", "/healthz"] # 有权访问的部分网址，如果需要访问其他 url 获取指标，应在此给予权限
  verbs: ["get"]

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: kube-system
----

$ kubectl create -f prometheus.rbac.yml

2、prometheus.config.yml configmap中的prometheus的配置文件
[source,bash]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config-v1
  namespace: kube-system
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
    scrape_configs:

    - job_name: 'kubernetes-kubelet'
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

    - job_name: 'kubernetes-cadvisor'
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

    - job_name: 'kubernetes-pod'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        regex: true
        action: keep # 筛选出带有 prometheus.io/scrape: 'true' 注释的 pod 获取指标。
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+) # 获取指标不是默认的 /metrics 自定义路径（别忘记添加上一步的 url 权限）
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__ # 指定端口

      - source_labels: ['__meta_kubernetes_pod_label_app', '__meta_kubernetes_pod_node_name']
        regex: 'node-exporter;(.*)'
        action: replace
        target_label: nodename

----
$ kubectl create -f prometheus.config.yml

3、prometheus.deploy.yml 定义 Prometheus 的部署： 这边有做固定到某个 node，先给这个 node 打标签 kubectl label nodes 172-25-134-47 app=prometheus(为了让pods固定在某个node上)
[source,bash]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: prometheus-deployment
  name: prometheus
  namespace: kube-system

spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      nodeSelector: # 选择带有这个标签的 node
        app: prometheus

      containers:
      - image: prom/prometheus:v2.2.1
        name: prometheus
        command:
        - "/bin/prometheus"
        args:
        - "--config.file=/etc/prometheus/prometheus.yml"
        - "--storage.tsdb.path=/prometheus"
        - "--storage.tsdb.retention=4h"
        ports:
        - containerPort: 9090
          protocol: TCP
        volumeMounts:
        - mountPath: "/prometheus" # 挂载数据
          name: data
        - mountPath: "/etc/prometheus" # 挂载配置
          name: config-volume
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
          limits:
            cpu: 500m
            memory: 2500Mi
      serviceAccountName: prometheus
      imagePullSecrets:
        - name: regsecret
      volumes:
      - name: data # 声明数据卷
        emptyDir: {}
      - name: config-volume # 声明配置卷
        configMap:
          name: prometheus-config-v1

----
$ kubectl create -f prometheus.deploy.yml

4、prometheus.svc.yml 定义 Prometheus 的 Service，需要将 Prometheus 以NodePort , LoadBalancer 或使用 Ingress 暴露到集群外部，这样外部的 Prometheus 才能访问它（这里采用 NodePort）
[source,bash]
----
kind: Service
apiVersion: v1
metadata:
  labels:
    app: prometheus
  name: prometheus
  namespace: kube-system
spec:
  type: NodePort
  ports:
  - port: 9090
    targetPort: 9090
    nodePort: 30003
  selector:
    app: prometheus

----
$ kubectl create -f prometheus.svc.yml


５、在 Kubernetes 上部署 node_exporter.yaml
[source,bash]
----
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: node-exporter
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      name: node-exporter
      labels:
        app: node-exporter
      annotations:
        prometheus.io/scrape: 'true' # 能被集群内 Prometheus 获取指标的关键表示符
        prometheus.io/port: '9100'
        prometheus.io/path: '/metrics'
    spec:
      tolerations: # 因 Master 节点具有 Taints（污点）属性，这里要设置 Tolerations（容忍）使得 Master 节点运行 node_exporter
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      volumes:
      - name: proc
        hostPath: {path: /proc}
      - name: sys
        hostPath: {path: /sys}
      containers:
      - name: node-exporter
        image: quay.io/prometheus/node-exporter:v0.15.2 # 因最新版 0.16 某些指标值有所改变，为了配合后面的 Grafana Dashboard，这边选用 0.15.2
        args: [--path.procfs=/proc_host, --path.sysfs=/host_sys]
        ports:
        - {name: node-exporter, hostPort: 9100, containerPort: 9100}
        volumeMounts:
        - {name: sys, readOnly: true, mountPath: /host_sys}
        - {name: proc, readOnly: true, mountPath: /proc_host}
        imagePullPolicy: IfNotPresent
      restartPolicy: Always
      hostNetwork: true
      hostPID: true
----
$ kubectl create -f node_exporter.yaml




６、在 Kubernetes 上部署 kube-state-metrics.yaml
$ vi kube-state-metrics.rbac.yaml
[source,bash]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: kube-system
---

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: kube-system
  name: kube-state-metrics-resizer
rules:
- apiGroups: [""]
  resources:
  - pods
  verbs: ["get"]
- apiGroups: ["extensions"]
  resources:
  - deployments
  resourceNames: ["kube-state-metrics"]
  verbs: ["get", "update"]
---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-state-metrics
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kube-state-metrics-resizer
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system
---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
  namespace: kube-system
rules:
- apiGroups: [""]
  resources:
  - configmaps
  - secrets
  - nodes
  - pods
  - services
  - resourcequotas
  - replicationcontrollers
  - limitranges
  - persistentvolumeclaims
  - persistentvolumes
  - namespaces
  - endpoints
  verbs: ["list", "watch"]
- apiGroups: ["extensions"]
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs: ["list", "watch"]
- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]
- apiGroups: ["batch"]
  resources:
  - cronjobs
  - jobs
  verbs: ["list", "watch"]
- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]
---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system


$ vi kube-state-metrics.deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: kube-system
spec:
  selector:
    matchLabels:
      "app": "kube-state-metrics"
  replicas: 1
  template:
    metadata:
      labels:
        "app": "kube-state-metrics"
      annotations:
        prometheus.io/scrape: 'true' # 能被集群内 Prometheus 获取指标的关键表示符
        prometheus.io/port: '8080'
        prometheus.io/path: '/metrics'
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: quay.io/coreos/kube-state-metrics:v1.3.1
        ports:
        - name: http-metrics
          containerPort: 8080
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
----
$ kubectl create -f kube-state-metrics.yaml


- 配置外部 Prometheus Federation

修改物理机上的Prometheus配置文件：vim /usr/local/prometheus/prometheus.yml

加上：scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: node03

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
    - targets: ['0.0.0.0:9090']

  - job_name: node04
    static_configs:
      - targets: ['172.16.1.221:9100']
        labels:
          instance: 172.16.1.221
  - job_name: 'federate'
    scrape_interval: 15s
    honor_labels: true
    metrics_path: '/federate'
    params:
      'match[]':
        - '{job=~"kubernetes-.*"}'
    static_configs:
      - targets:
        - '172.16.1.220:30003'



配置grafana发邮件：
vim /etc/grafana/grafana.ini
  [smtp]
;enabled = false
;host = localhost:25
;user =
# If the password contains # or ; you have to wrap it with trippel quotes. Ex """#password;"""
;password =
;cert_file =
;key_file =
;skip_verify = false
;from_address = admin@grafana.localhost
;from_name = Grafana
# EHLO identity in SMTP dialog (defaults to instance_name)
;ehlo_identity = dashboard.example.com
enabled = true
host = smtp.exmail.qq.com:25
user = mwteck@mwteck.com
password = xxxxxxxx
skip_verift = true
from_address = mwteck@mwteck.com
from_name = Grafana




配置grafana的数据源链接到172.16.1.220:30003就ＯＫ啦
