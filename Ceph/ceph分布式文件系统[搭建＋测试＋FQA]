== ceph(分布式文件系统)

.参考文档：
http://docs.ceph.org.cn/start/intro/
https://github.com/wzyuliyang/testMarkdown/blob/master/rgw%E7%BD%91%E5%85%B3.markdown

- 部署说明：
     主机操作系统 CentOS Linux release 7.4.1708 (Core)
     此次部署将有一个ceph-deploy管理节点和三个ceph存储集群节点,一个客户端.
#集群基础模型～至于后面怎么加功能块看自己～各个节点都可以加
主机名            IP                       扮演角色
node1            172.16.3.38              mon
node2            172.16.3.39              osd
node3            172.16.3.40              osd
client-node      172.16.3.41              测试存储
admin-node      172.16.3.37              admin

- 环境准备：
1.分别修改各个主机名和hosts文件，方便自己理解各个主机的作用和中途的操作。
[source,bash]
----
hostnamectl set-hostname yourname
vim /etc/hosts
----
分别在里面加入
[source,text]
----
172.16.3.38 node1
172.16.3.39 node2
172.16.3.40 node3
172.16.3.41 client-node
172.16.3.42 admin-node
----

分别安装NTP同步时间并同步时间
[source,bash]
----
[root@admin-node ~]# yum -y install ntp ntpdate ntp-doc

[root@admin-node ~]# ntpdate  202.120.2.101
 1 Mar 11:25:25 ntpdate[2740]: adjust time server 202.120.2.101 offset -0.006555 sec
----

本次实验为了方便直接使用的root账户，一般是用自己创建的账户，只需要修改配置文件（/etc/sudoers）获取root权限就行。

允许无密码ssh登陆

创建密钥
[source,bash]
----
[root@admin-node ~]# ssh-keygen
……#全程回车即可
[root@admin-node ~]# cd  ./.ssh/
[root@admin-node .ssh]# ls
authorized_keys  id_rsa  id_rsa.pub  known_hosts
----
分发密钥到各个节点
[source,bash]
----
[root@admin-node .ssh]# ssh-copy-id node1     #第一次直接输入yes然后输入节点密码就行
[root@admin-node .ssh]# ssh-copy-id node2
[root@admin-node .ssh]# ssh-copy-id node3
[root@admin-node .ssh]# ssh-copy-id client-node
----
（推荐做法）修改 ceph-deploy 管理节点上的 ~/.ssh/config 文件，这样 ceph-deploy 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 ceph-deploy 都要指定 --username {username} 。这样做同时也简化了 ssh 和 scp 的用法。
[source,bash]
----
[root@admin-node .ssh]# cat config
Host node1
   Hostname node1
   User root
Host node2
   Hostname node2
   User root
Host node3
   Hostname node3
   User root
Host client-node
   Hostname client-node
   User root
----

关闭防火墙和SELINUX
[source,bash]
----
[root@admin-node ~]# setenforce 0
[root@admin-node ~]# systemctl stop firewalld || systemctl disable firewalld
----

.安装 ceph 部署工具
把 Ceph 仓库添加到 ceph-deploy 管理节点，然后安装 ceph-deploy 。
[source,bash]
----
yum install -y yum-utils && sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ && sudo yum install --nogpgcheck -y epel-release && sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 && sudo rm /etc/yum.repos.d/dl.fedoraproject.org*
----

把软件包源加入软件仓库，用文本编辑器写一个yum库文件
[source,bash]
----
vim /etc/yum.repos.d/ceph.repo
----

把如下内容粘帖进去，用 Ceph 的最新主稳定版名字替换 {ceph-stable-release} （如 firefly ），用你的Linux发行版名字替换 {distro} （如 el6 为 CentOS 6 、 el7 为 CentOS 7 、 rhel6 为 Red Hat 6.5 、 rhel7 为 Red Hat 7 、 fc19 是 Fedora 19 、 fc20 是 Fedora 20 ）。最后保存到 /etc/yum.repos.d/ceph.repo 文件中。

[source,text]
----
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-{ceph-release}/{distro}/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
----
更新软件库并安装 ceph-deploy
[source,bash]
----
yum update && sudo yum install ceph-deploy
----

先在管理节点上创建一个目录，用于保存 ceph-deploy 生成的配置文件和密钥对。

[root@admin-node ~]# mkdir ceph-cluster
[root@admin-node ~]# cd ceph-cluster/

创建集群
在管理节点上，进入刚创建的放置配置文件的目录，用 ceph-deploy 执行如下步骤
----
[root@admin-node ceph-cluster]# ceph-deploy new node1
[root@admin-node ceph-cluster]# ls
ceph.conf  ceph.log  ceph.mon.keyring
[root@admin-node ceph-cluster]# vim ceph.conf
[root@admin-node ceph-cluster]# cat  ceph.conf
[global]
fsid = 97f00feb-29c9-43de-9460-d2bf6a0799b9
mon_initial_members = node1
mon_host = 172.16.3.38
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
filestore_xattr_use_omap = true

osd pool default size = 2

----

在各个节点安装ceph
ceph-deploy install admin-node node1 node2 node3            #建议把各个机子的环境配好，先执行yum -y install ceph-deploy ceph ceph-radosgw

----
[root@admin-node ceph-cluster]# ceph-deploy install admin-node node1 node2 node3 client-node
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy install admin-node node1 node2 node3 client-node
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x1bd4f80>
…………
…………
[client-node][DEBUG ] 12 packages excluded due to repository priority protections
[client-node][DEBUG ] Package 1:ceph-10.2.10-0.el7.x86_64 already installed and latest version
[client-node][DEBUG ] Package 1:ceph-radosgw-10.2.10-0.el7.x86_64 already installed and latest version
[client-node][DEBUG ] Nothing to do
[client-node][INFO  ] Running command: ceph --version
[client-node][DEBUG ] ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)

----






配置初始monitors,并收集所有密钥
ceph-deploy mon create-initial

----
[root@admin-node ceph-cluster]# ceph-deploy mon create-initial
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[ceph_deploy.cli][INFO  ] ceph-deploy options:
…………
…………
[node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-node1/keyring auth get client.bootstrap-rgw
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmphcrNWu

----

ssh node2
sudo mkdir /var/local/osd0
chown ceph:ceph /var/local/osd0
exit

ssh node3
sudo mkdir /var/local/osd1
chown ceph:ceph /var/local/osd1
exit

准备
[source,bash]
----
ssh node2
mkdir /var/local/osd0
chown ceph:ceph /var/local/osd0
exit

ssh node3
mkdir /var/local/osd1
chown ceph:ceph /var/local/osd1
exit

[root@admin-node ceph-cluster]# ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  block_db                      : None
…………
…………
[node3][WARNIN] command: Running command: /usr/bin/chown -R ceph:ceph /var/local/osd1/magic.999.tmp
[node3][INFO  ] checking OSD status...
[node3][DEBUG ] find the location of an executable
[node3][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node3 is now ready for osd use.

----


激活
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
…………
…………
[node3][WARNIN] command_check_call: Running command: /usr/bin/systemctl start ceph-osd@1
[node3][INFO  ] checking OSD status...
[node3][DEBUG ] find the location of an executable
[node3][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json
[node3][INFO  ] Running command: systemctl enable ceph.target
----


用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了。
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy admin admin-node node1 node2 node3
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin admin-node node1 node2 node3
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x2195908>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['admin-node', 'node1', 'node2', 'node3']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f3cd488c8c0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to admin-node
[admin-node][DEBUG ] connected to host: admin-node
[admin-node][DEBUG ] detect platform information from remote host
[admin-node][DEBUG ] detect machine type
[admin-node][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node1
[node1][DEBUG ] connected to host: node1
[node1][DEBUG ] detect platform information from remote host
[node1][DEBUG ] detect machine type
[node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node2
[node2][DEBUG ] connected to host: node2
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node3
[node3][DEBUG ] connected to host: node3
[node3][DEBUG ] detect platform information from remote host
[node3][DEBUG ] detect machine type
[node3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf

----

确保你对 ceph.client.admin.keyring 有正确的操作权限。
[source,bash]
----
[root@admin-node ceph-cluster]# chmod +r /etc/ceph/ceph.client.admin.keyring
……
[root@node1 ~]# chmod +r /etc/ceph/ceph.client.admin.keyring
[root@node1 ~]# ls /etc/ceph/ceph.client.admin.keyring -l
-rw-r--r--. 1 root root 129 Mar  2 10:16 /etc/ceph/ceph.client.admin.keyring
----

查看集群的状态
[source,bash]
----
[root@admin-node ceph-cluster]# ceph -s
    cluster 25319ec9-a963-4759-9fe7-9f8f37b64c90
     health HEALTH_OK
     monmap e1: 1 mons at {node1=172.16.3.38:6789/0}
            election epoch 3, quorum 0 node1
     osdmap e10: 2 osds: 2 up, 2 in
            flags sortbitwise,require_jewel_osds
      pgmap v24: 64 pgs, 1 pools, 0 bytes data, 0 objects
            13320 MB used, 43980 MB / 57300 MB avail
                  64 active+clean
[root@admin-node ceph-cluster]# ceph health
HEALTH_OK
----

- 拓展集群
[source,bash]
----
[root@admin-node ceph-cluster]# ssh node1
Last login: Fri Mar  2 08:27:11 2018 from 172.16.3.30
[root@node1 ~]# mkdir /var/local/osd2
[root@node1 ~]# chown ceph:ceph /var/local/osd2
[root@node1 ~]# exit
logout
Connection to node1 closed.
[root@admin-node ceph-cluster]# ceph-deploy osd prepare node1:/var/local/osd2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare node1:/var/local/osd2
[node1][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node1 is now ready for osd use.
[root@admin-node ceph-cluster]# ceph-deploy osd activate node1:/var/local/osd2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd activate node1:/var/local/osd2

----



[source,bash]
----
[root@admin-node ceph-cluster]# ceph -s
    cluster 25319ec9-a963-4759-9fe7-9f8f37b64c90
     health HEALTH_OK
     monmap e1: 1 mons at {node1=172.16.3.38:6789/0}
            election epoch 3, quorum 0 node1
     osdmap e15: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v59: 64 pgs, 1 pools, 0 bytes data, 0 objects
            19985 MB used, 65964 MB / 85950 MB avail
                  64 active+clean


----


添加元数据服务器，暂时官方不提供多个元数据服务器技术支持
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy mds create node1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mds create node1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x1150200>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function mds at 0x10ef5f0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  mds                           : [('node1', 'node1')]
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.mds][DEBUG ] Deploying mds, cluster ceph hosts node1:node1
[node1][DEBUG ] connected to host: node1
[node1][DEBUG ] detect platform information from remote host
[node1][DEBUG ] detect machine type
[ceph_deploy.mds][INFO  ] Distro info: CentOS Linux 7.4.1708 Core
[ceph_deploy.mds][DEBUG ] remote host will use systemd
[ceph_deploy.mds][DEBUG ] deploying mds bootstrap to node1
[node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node1][DEBUG ] create path if it doesn't exist
[node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mds --keyring /var/lib/ceph/bootstrap-mds/ceph.keyring auth get-or-create mds.node1 osd allow rwx mds allow mon allow profile mds -o /var/lib/ceph/mds/ceph-node1/keyring
[node1][INFO  ] Running command: systemctl enable ceph-mds@node1
[node1][WARNIN] Created symlink from /etc/systemd/system/ceph-mds.target.wants/ceph-mds@node1.service to /usr/lib/systemd/system/ceph-mds@.service.
[node1][INFO  ] Running command: systemctl start ceph-mds@node1
[node1][INFO  ] Running command: systemctl enable ceph.target

----


添加对象网关
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy rgw create node1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy rgw create node1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  rgw                           : [('node1', 'rgw.node1')]
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x18f5170>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function rgw at 0x18396e0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts node1:rgw.node1
[node1][DEBUG ] connected to host: node1
[node1][DEBUG ] detect platform information from remote host
[node1][DEBUG ] detect machine type
[ceph_deploy.rgw][INFO  ] Distro info: CentOS Linux 7.4.1708 Core
[ceph_deploy.rgw][DEBUG ] remote host will use systemd
[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to node1
[node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node1][DEBUG ] create path recursively if it doesn't exist
[node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.node1 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.node1/keyring
[node1][INFO  ] Running command: systemctl enable ceph-radosgw@rgw.node1
[node1][WARNIN] Created symlink from /etc/systemd/system/ceph-radosgw.target.wants/ceph-radosgw@rgw.node1.service to /usr/lib/systemd/system/ceph-radosgw@.service.
[node1][INFO  ] Running command: systemctl start ceph-radosgw@rgw.node1
[node1][INFO  ] Running command: systemctl enable ceph.target
[ceph_deploy.rgw][INFO  ] The Ceph Object Gateway (RGW) is now running on host node1 and default port 7480

----

RGW 例程默认会监听 7480 端口，可以更改该节点 ceph.conf 内与 RGW 相关的配置，如下
[source,bash]
----
[root@admin-node ceph-cluster]# cat  ceph.conf
[global]
fsid = 25319ec9-a963-4759-9fe7-9f8f37b64c90
mon_initial_members = node1
mon_host = 172.16.3.38
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
osd pool default size = 2


[client]
rgw frontends = civetweb port=80
----



[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy mon add node2 node3
usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                   [--overwrite-conf] [--ceph-conf CEPH_CONF]
                   COMMAND ...
ceph-deploy: error: unrecognized arguments: node3

----


块设备的使用
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy install client-node
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy install client-node
[ceph_deploy.cli][INFO  ] ceph-deploy options:
…………
…………
[root@admin-node ceph-cluster]# ceph-deploy admin client-node
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client-node
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x1a0b908>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['client-node']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fef43af68c0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client-node
[client-node][DEBUG ] connected to host: client-node
[client-node][DEBUG ] detect platform information from remote host
[client-node][DEBUG ] detect machine type
[client-node][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[root@admin-node ceph-cluster]# ssh client-node
Last login: Fri Mar  2 08:28:07 2018 from 172.16.3.30
[root@client-node ~]# chmod +r /etc/ceph/ceph.client.admin.keyring
[root@client-node ~]# rbd create rbd/kuai-db2 --size 10G --image-format 2 --image-feature  layering
[root@client-node ~]# rbd ls
kuai-db
kuai-db2
[root@client-node ~]# rbd map rbd/kuai-db2
/dev/rbd1
[root@client-node ~]# mkdir /mnt/Block-device
[root@client-node ~]# mkfs.ext4 -m0 /dev/rbd1
mke2fs 1.42.9 (28-Dec-2013)
Discarding device blocks: done
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=1024 blocks, Stripe width=1024 blocks
655360 inodes, 2621440 blocks
0 blocks (0.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=2151677952
80 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks:
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done
Writing inode tables: done
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done

[root@client-node ~]# lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   30G  0 disk
├─sda1            8:1    0    1G  0 part /boot
└─sda2            8:2    0   29G  0 part
  ├─centos-root 253:0    0   28G  0 lvm  /
  └─centos-swap 253:1    0    1G  0 lvm  [SWAP]
sr0              11:0    1 1024M  0 rom
rbd0            252:0    0   10G  0 disk
rbd1            252:16   0   10G  0 disk /mnt/Block-device

----


文件系统：
----
[root@client-node Block-device]# ceph osd pool create cephfs_data 256 256
pool 'cephfs_data' created
[root@client-node Block-device]# ceph osd pool create cephfs_metadata 256 256
pool 'cephfs_metadata' created
[root@client-node Block-device]# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 9 and data pool 8
[root@client-node Block-device]# ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
[root@client-node Block-device]# ceph mds stat
e5: 1/1/1 up {0=node1=up:active}


[root@client-node ~]# ceph-fuse -m node1:6789 /mnt/cephFS
ceph-fuse[1763]: starting ceph client
2018-03-02 13:53:00.734996 7fc6c36c2ec0 -1 init, newargv = 0x562af0a827e0 newargc=11
ceph-fuse[1763]: starting fuse

#等一会儿就好了？ lsblk看不到是因为这个命令是只看块设备的ge  但是df -h 可以


----

对象存储

----
[root@admin-node ceph-cluster]# ceph-deploy install --rgw node3   #安装对象网关
[root@admin-node ceph-cluster]# ceph-deploy admin node3    #设置节点管理权限
[root@admin-node ceph-cluster]# ceph-deploy rgw create node3 #新建网关实例

为了使用 REST 接口，首先需要为S3接口创建一个初始 Ceph 对象网关用户。然后，为 Swift 接口创建一个子用户。然后你需要验证创建的用户是否能够访问网关。

为 S3 访问创建 RADOSGW 用户
一个``radosgw`` 用户需要被新建并被分配权限。命令 man radosgw-admin 会提供该命令的额外信息。
[root@node3 ~]# sudo radosgw-admin user create --uid="testuser" --display-name="First User"

命令的输出跟下面的类似:
[source,text]
----
{
        "user_id": "testuser",
        "display_name": "First User",
        "email": "",
        "suspended": 0,
        "max_buckets": 1000,
        "auid": 0,
        "subusers": [],
        "keys": [{
                "user": "testuser",
                "access_key": "I0PJDPCIYZ665MW88W9R",
                "secret_key": "dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA"
        }],
        "swift_keys": [],
        "caps": [],
        "op_mask": "read, write, delete",
        "default_placement": "",
        "placement_tags": [],
        "bucket_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "user_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "temp_url_keys": []
}

Note 其中 keys->access_key 和``keys->secret_key`` 的值在访问的时候需要用来做验证
----
新建一个 SWIFT 用户
如果你想要使用这种方式访问集群，你需要新建一个 Swift 子用户。创建 Swift 用户包括两个步骤。第一步是创建用户。第二步是创建 secret key。
[root@node3 ~]# adosgw-admin subuser create --uid=testuser --subuser=testuser:swift --access=full
输出类似下面这样:
[source,text]
----
{
        "user_id": "testuser",
        "display_name": "First User",
        "email": "",
        "suspended": 0,
        "max_buckets": 1000,
        "auid": 0,
        "subusers": [{
                "id": "testuser:swift",
                "permissions": "full-control"
        }],
        "keys": [{
                "user": "testuser:swift",
                "access_key": "3Y1LNW4Q6X0Y53A52DET",
                "secret_key": ""
        }, {
                "user": "testuser",
                "access_key": "I0PJDPCIYZ665MW88W9R",
                "secret_key": "dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA"
        }],
        "swift_keys": [],
        "caps": [],
        "op_mask": "read, write, delete",
        "default_placement": "",
        "placement_tags": [],
        "bucket_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "user_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "temp_url_keys": []
 }
----
[root@node3 ~]#sudo radosgw-admin key create --subuser=testuser:swift --key-type=swift --gen-secret
[source,text]
----
{
        "user_id": "testuser",
        "display_name": "First User",
        "email": "",
        "suspended": 0,
        "max_buckets": 1000,
        "auid": 0,
        "subusers": [{
                "id": "testuser:swift",
                "permissions": "full-control"
        }],
        "keys": [{
                "user": "testuser:swift",
                "access_key": "3Y1LNW4Q6X0Y53A52DET",
                "secret_key": ""
        }, {
                "user": "testuser",
                "access_key": "I0PJDPCIYZ665MW88W9R",
                "secret_key": "dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA"
        }],
        "swift_keys": [{
                "user": "testuser:swift",
                "secret_key": "244+fz2gSqoHwR3lYtSbIyomyPHf3i7rgSJrF\/IA"
        }],
        "caps": [],
        "op_mask": "read, write, delete",
        "default_placement": "",
        "placement_tags": [],
        "bucket_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "user_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "temp_url_keys": []
}
----


----
查看本机某用户的信息
[root@node3 ~]# ceph -s  #看健康状况
[root@node3 ~]# ceph df  #看存储量大小
[root@node3 ~]# radosgw-admin user info --uid=testuser    #查看网关用户信息
{
    "user_id": "testuser",
    "display_name": "First User",
    "email": "",
    "suspended": 0,
    "max_buckets": 1000,
    "auid": 0,
    "subusers": [
        {
            "id": "testuser:swift",
            "permissions": "full-control"
        }
    ],
    "keys": [
        {
            "user": "testuser",
            "access_key": "GWHSA3ZECQ9COFG6UK9J",
            "secret_key": "fnartFbney3zsDQOxABk16PdAOtVP1acnFoOgU8P"
        }
    ],
    "swift_keys": [
        {
            "user": "testuser:swift",
            "secret_key": "c8CSYQclJPojcXs8gD9lgjrtJfBTpbbElrK5dYJg"
        }
    ],
    "caps": [],
    "op_mask": "read, write, delete",
    "default_placement": "",
    "placement_tags": [],
    "bucket_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    },
    "user_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    },
    "temp_url_keys": []
}

----

测试 S3 访问
为了验证 S3 访问，你需要编写并运行一个 Python 测试脚本。S3 访问测试脚本将连接 radosgw, 新建一个新的 bucket 并列出所有的 buckets。 aws_access_key_id 和 aws_secret_access_key 的值来自于命令``radosgw_admin`` 的返回值 access_key 和 secret_key 。
[root@node3 ~]# yum install python-boto

[root@node3 ~]# vi s3test.py
import boto
import boto.s3.connection
#模板
access_key = 'I0PJDPCIYZ665MW88W9R'
secret_key = 'dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA'
conn = boto.connect_s3(
        aws_access_key_id = access_key,
        aws_secret_access_key = secret_key,
        host = '{hostname}', port = {port},
        is_secure=False, calling_format = boto.s3.connection.OrdinaryCallingFormat(),
        )

bucket = conn.create_bucket('my-new-bucket')
    for bucket in conn.get_all_buckets():
            print "{name}".format(
                    name = bucket.name,
                    created = bucket.creation_date,
 )
#实际
[root@node3 ~]# cat s3test.py
import boto
import boto.s3.connection

access_key = 'GWHSA3ZECQ9COFG6UK9J'
secret_key = 'fnartFbney3zsDQOxABk16PdAOtVP1acnFoOgU8P'
conn = boto.connect_s3(
        aws_access_key_id = access_key,
        aws_secret_access_key = secret_key,
        host = '172.16.3.40', port =7480,
        is_secure=False, calling_format = boto.s3.connection.OrdinaryCallingFormat(),
        )

bucket = conn.create_bucket('my-01-bucket')
for bucket in conn.get_all_buckets():
            print "{name}".format(name = bucket.name,created = bucket.creation_date,)

#注意修改host和端口

[root@node3 ~]# python s3test.py
my-new-bucket                    #即为验证成功

……
……
[root@node3 ~]# radosgw-admin bucket list  #查看所有的bucket
[
    "my-new-bucket",
    "my-01-bucket"
]


#管理并操作bucket
[root@node3 ~]# vi ~/.s3cfg
//修改 access_key  host_base  host_bucket  secret_key

access_key = GWHSA3ZECQ9COFG6UK9J
secret_key = fnartFbney3zsDQOxABk16PdAOtVP1acnFoOgU8P
host_base = node3          #如果这里你的端口为80可以不管，如果为7480（默认）要写成：host_base = node3:7480
host_bucket = %(*)s.node3  #这儿这个括号里面的要么为自己的bucket名称要么为×号（×号为所有的bucket）

#显示全部bucket名称
[root@node3 ~]# s3cmd ls
2018-03-21 09:14  s3://my-01-bucket
2018-03-22 16:24  s3://testdb-bucket

#创建 bucket，且 bucket 名称是唯一的，不能重复。
[root@node3 ~]# s3cmd mb s3://my-bucket-name

#删除空 bucket
[root@node3 ~]# s3cmd rb s3://my-bucket-name

#列举 Bucket 中的内容
[root@node3 ~]# s3cmd ls s3://my-bucket-name

#上传 file.txt 到某个 bucket，
[root@node3 ~]# s3cmd put file.txt s3://my-bucket-name/file.txt

#上传并将权限设置为所有人可读
[root@node3 ~]# s3cmd put --acl-public file.txt s3://my-bucket-name/file.txt

eg:
[root@node3 ~]# s3cmd put --acl-public anaconda-ks.cfg s3://testdb-bucket/
upload: 'anaconda-ks.cfg' -> 's3://testdb-bucket/anaconda-ks.cfg'  [1 of 1]
 1218 of 1218   100% in    0s    16.56 kB/s  done
Public URL of the object is: http://node3:7480/testdb-bucket/anaconda-ks.cfg
上传的文件是可以直接通过网络访问的

#批量上传文件
[root@node3 ~]# s3cmd put ./* s3://my-bucket-name/

#下载文件
[root@node3 ~]# s3cmd get s3://my-bucket-name/file.txt file.txt

#批量下载
[root@node3 ~]# s3cmd get s3://my-bucket-name/* ./

#删除文件
[root@node3 ~]# s3cmd del s3://my-bucket-name/file.txt

#来获得对应的bucket所占用的空间大小
[root@node3 ~]# s3cmd du -H s3://my-bucket-name





----


.FAQ：
1. 为什么我的源总是出问题？
  因为一开始源的配置就没弄好，可以参考官方文档里面的配置ceph.repo源文件。
  http://docs.ceph.org.cn/start/quick-start-preflight/#ceph




2.RuntimeError: NoSectionError: No section: 'ceph' 【没有找到ceph】
[ceph_node01][DEBUG ] ceph-release-1-1.el7                  ########################################
[ceph_node01][WARNIN] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[ceph_deploy][ERROR ] RuntimeError: NoSectionError: No section: 'ceph'
…………
[ceph_node01][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: yum -y install ceph ceph-radosgw

问题所在：可能是因为网络原因，导致admin端没成功给node1安装ceph ceph-radosgw导致的,建议直接自己手动在node1安装

[ceph@ceph_node01 my-cluster]$ sudo yum -y install ceph ceph-radosgw
Loaded plugins: fastestmirror, priorities
Repository base is listed more than once in the configuration





3、安装报错，缺包
--> Finished Dependency Resolution
Error: Package: 1:ceph-common-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libbabeltrace-ctf.so.1()(64bit)
Error: Package: 1:ceph-mon-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libleveldb.so.1()(64bit)
Error: Package: 1:librbd1-10.2.10-0.el7.x86_64 (Ceph)
           Requires: liblttng-ust.so.0()(64bit)
Error: Package: 1:ceph-common-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libbabeltrace.so.1()(64bit)
Error: Package: 1:librgw2-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libfcgi.so.0()(64bit)
Error: Package: 1:ceph-base-10.2.10-0.el7.x86_64 (Ceph)
           Requires: liblttng-ust.so.0()(64bit)
Error: Package: 1:ceph-osd-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libleveldb.so.1()(64bit)
Error: Package: 1:librados2-10.2.10-0.el7.x86_64 (Ceph)
           Requires: liblttng-ust.so.0()(64bit)
Error: Package: 1:ceph-radosgw-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libfcgi.so.0()(64bit)
Error: Package: 1:ceph-radosgw-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libfcgi.so.0()(64bit)
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
没有yum的epel-release源导致的，缺libfcgi.so的包
建议手动安装
yum -y install epel-release



4、源的地址没有找到
[admin-node][INFO  ] Running command: rpm --import https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc
[admin-node][INFO  ] Running command: rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[admin-node][DEBUG ] Retrieving http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[admin-node][WARNIN] error: open of <html> failed: No such file or directory
[admin-node][WARNIN] error: open of <head><title>Index failed: No such file or directory
[admin-node][WARNIN] error: open of of failed: No such file or directory
…………
[admin-node][ERROR ] RuntimeError: command returned non-zero exit status: 32
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm

源有问题建议换一个 例如：http://ceph.com/rpm-hammer/el7/noarch




5、个别节点down了
[root@admin-node ceph-cluster]# ceph osd tree
ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.08189 root default
-2 0.02730     host node2
 0 0.02730         osd.0       up  1.00000          1.00000
-3 0.02730     host node3
 1 0.02730         osd.1     down        0          1.00000
-4 0.02730     host node1
 2 0.02730         osd.2       up  1.00000        node02
[ceph_node02][DEBUG ] detect platform information from remote hosts  1.00000
-5       0     host ttd

 这个问题在于osd.1节点down掉了，我们重新启动起来就好

 [root@admin-node ceph-cluster]# ceph-deploy osd activate  node3:/var/local/osd1




6、[ceph@ceph_node01 my-cluster]$ sudo ceph-deploy mon create-initial
[sudo] password for ceph:
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy mon create-initial
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x1198908>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x118e5f0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  keyrings                      : None
[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts ceph_node02
[ceph_deploy.mon][DEBUG ] detecting platform for host ceph_node02 ...
[ceph_node02][DEBUG ] connected to host: ceph_node02
[ceph_node02][DEBUG ] detect platform information from remote hosts
[ceph_node02][DEBUG ] detect machine type
[ceph_node02][DEBUG ] find the location of an executable
[ceph_deploy.mon][ERROR ] ceph needs to be installed in remote host: ceph_node02
[ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

“这个做测试的时候忘了怎么解决的。。”



7、随便执行什么ceph的命令就一直弹这个报错
[root@node3 ~]# ceph -s
2018-03-21 11:52:18.676487 7f2f8c7e2700  0 -- :/785802200 >> 172.16.3.38:6789/0 pipe(0x7f2f88064170 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7f2f8805c7d0).fault
2018-03-21 11:52:21.677960 7f2f8c6e1700  0 -- :/785802200 >> 172.16.3.38:6789/0 pipe(0x7f2f7c000c80 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7f2f7c001f90).fault
^C2018-03-21 11:52:24.678869 7f2f8c7e2700  0 -- :/785802200 >> 172.16.3.38:6789/0 pipe(0x7f2f7c0052b0 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7f2f7c006570).fault
问题所在：防火墙的问题，关了就OK
解决：各个节点执行
[root@admin-node ~]# iptables -F
[root@admin-node ~]# systemctl stop firewalld
[root@admin-node ~]# systemctl enable firewalld



8、链接拒绝
[root@node3 ~]# python s3test1.py
Traceback (most recent call last):
  File "s3test1.py", line 12, in <module>
    bucket = conn.create_bucket('my-new-bucket')
  File "/usr/lib/python2.7/site-packages/boto/s3/connection.py", line 615, in create_bucket
    response = self.make_request('PUT', bucket_name, headers=headers,data=data)
  File "/usr/lib/python2.7/site-packages/boto/s3/connection.py", line 667, in make_request
    retry_handler=retry_handler
  File "/usr/lib/python2.7/site-packages/boto/connection.py", line 1071, in make_request
    retry_handler=retry_handler)
  File "/usr/lib/python2.7/site-packages/boto/connection.py", line 1030, in _mexe
    raise ex
socket.error: [Errno 111] Connection refused
 问题所在：7480端口没启动
[root@node3 ~]# ss -ntl
State      Recv-Q Send-Q                                  Local Address:Port                                                 Peer Address:Port
LISTEN     0      128                                                 *:6800                                                            *:*
LISTEN     0      128                                       172.16.3.40:6804                                                            *:*
LISTEN     0      128                                       172.16.3.40:6805                                                            *:*
LISTEN     0      128                                       172.16.3.40:6806                                                            *:*
LISTEN     0      128                                                 *:22                                                              *:*
LISTEN     0      100                                         127.0.0.1:25                                                              *:*
LISTEN     0      100                                                :::8080                                                           :::*
LISTEN     0      128                                                :::22                                                             :::*
LISTEN     0      100                                               ::1:25                                                             :::*
LISTEN     0      1                                    ::ffff:127.0.0.1:8005                                                           :::*
LISTEN     0      100                                                :::8009

解决方法：重启服务
[root@node3 ~]# systemctl restart ceph-radosgw.target


9、 健康警告
[root@node1 ~]# ceph -s
    cluster 25319ec9-a963-4759-9fe7-9f8f37b64c90
     health HEALTH_WARN
            too many PGs per OSD (437 > max 300)
     monmap e1: 1 mons at {node1=172.16.3.38:6789/0}
            election epoch 9, quorum 0 node1
      fsmap e35: 1/1/1 up {0=node1=up:active}
     osdmap e322: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v87195: 656 pgs, 13 pools, 136 MB data, 264 objects
            30731 MB used, 55218 MB / 85950 MB avail
                 656 active+clean
                                               :::*
问题原因是集群osd 数量较少，在我的测试过程中，由于搭建rgw网关、和OpenStack集成等，创建了大量的pool，每个pool要占用一些pg ，ceph集群默认每块磁盘都有默认值，好像每个osd 为300个pgs，不过这个默认值是可以调整的，但调整得过大或者过小都会对集群的性能产生一定影响。因为我们这个是测试环境，只要能消除掉报错即可。查询当前每个osd下最大的pg报警值：

$ ceph --show-config  | grep mon_pg_warn_max_per_osd
p.s:如果没写默认是300,我们修改了就ok
在mon节点的ceph.conf（/etc/ceph/ceph.conf）配置文件中添加:

$ vi /etc/ceph/ceph.conf
[global]
.......
mon_pg_warn_max_per_osd = 1000
重启monitor服务：

$ systemctl restart ceph-mon.target


10、[root@node3 ~]# s3cmd ls
ERROR: S3 error: 403 (RequestTimeTooSkewed): The difference between the request time and the current time is too large.
问题所在：时间不对



11、[root@node3 ~]# python news3.py
Traceback (most recent call last):
  File "news3.py", line 13, in <module>
    bucket = conn.create_bucket('my-new-bucket')
  File "/usr/lib/python2.7/site-packages/boto/s3/connection.py", line 619, in create_bucket
    response.status, response.reason, body)
boto.exception.S3CreateError: S3CreateError: 409 Conflict
<?xml version="1.0" encoding="UTF-8"?><Error><Code>BucketAlreadyExists</Code><BucketName>my-new-bucket</BucketName><RequestId>tx000000000000000000030-005ab3e75a-1217d-default</RequestId><HostId>1217d-default-default</HostId></Error>
问题所在：BucketAlreadyExists 在ceph的使用过程中bucket的名称必须是唯一的，这里的报错就是因为名称已经存在了。
解决方法：改一下测试脚本里面的mucket名称即可。


12、[root@node3 ~]# s3cmd ls

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    An unexpected error has occurred.
  Please try reproducing the error using
  the latest s3cmd code from the git master
  branch found at:
    https://github.com/s3tools/s3cmd
  and have a look at the known issues list:
    https://github.com/s3tools/s3cmd/wiki/Common-known-issues-and-their-solutions
  If the error persists, please report the
  following lines (removing any private
  info as necessary) to:
   s3tools-bugs@lists.sourceforge.net


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Invoked as: /usr/bin/s3cmd ls
Problem: error: [Errno 111] Connection refused
S3cmd:   1.6.1
python:   2.7.5 (default, Nov 20 2015, 02:00:19)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]
environment LANG=zh_CN.UTF-8

问题所在： 请注意 报错 ：Problem: error: [Errno 111] Connection refused 。这种链接被拒绝一般是注意两个点：地址+端口。通过打印debug信息发现我们/root/.s3cfg里面寻找的端口是80，而我的端口是7480（这个端口号看你自己的端口是多少，默认为7480，一般根据官方文档做的可能为80）
解决方法：
修改/root/.s3cfg文件里面的配置为：host_base = node3:7480
