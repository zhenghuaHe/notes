= 学习笔记整理

== 1.装机
=== 1.1 windows
==== 1.1.1 windows注意磁盘格式（MBR or GPT）

- 常见报错：无法将Win7安装到本磁盘中（GPT分区）



  win7安装需要硬盘位MBR个，现在很多预装win10的系统默认都是GPT格式，使用以下方式可以实现各种硬盘格式的相互转换。

  当出现"您想将windows安装在何处时"，按shift+F10打开命令提示符，输入：
  diskpart
  list disk
  sel disk 0
  clean
  convert mbr
  即可完成其他硬盘格式到MBR格式转换，完成转化后需要重新分区。


注意 ：

  sel  disk 0 意思是选中select  硬盘盘符为disk 0硬盘分区，根据需要和硬盘分区不同可选择disk1 , disk2,,,, 等其他盘符。

  如果要转变为GPT格式，最后一行改为 convert GPT就可以了。


- 常见报错：加载驱动程序，找不到CD/DVD驱动器设备驱动程序。

  解决方法：问题根源是系统没有认出你的U盘来，只需要让它认出来就好啦。
  方法一：确认你的U盘接口和笔记本的接口是否吻合（2.0和3.0的有时候读不出来）
  方法二：重启大法好

==== 1.1.2 安装好之后的细节操作

  1. 激活系统
  2. 关闭windows的update更新服务
  3. 用自带无线网卡驱动/网卡驱动的驱动机灵安装必要的驱动
  4. 安装必要的软件。WinRAR ZIP之类的解压软件，Microsoft系列的办公套间
  5. 创建一个还原点





=== 1.2 archlinux
  archlinux系统的安装相当于完全定制的，顺序：U盘写入-->进入系统进行定制自己的archlinux
  操作步骤可以参考：
  1.官方文档：
  https://wiki.archlinux.org/index.php/Installation_guide_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)
  2.网友博客：
  http://blog.csdn.net/r8l8q8/article/details/76516523

  FAQ：

  1. 用写好的U盘系统安装arch,进入后什么都没有，只有个小光标一闪一闪？

      进入系统后屏幕只有一个小光标闪烁那就是没找到里面的启动信息，所以就什么都没有，我们应该在u盘写入时，写入方式选择raw。再进去安装系统就不会没有引导信息。

  2. 为什么我根据教程完成了自定义的archlinux系统，重启之后就什么都没有了？

  注意bios里面启动格式是否设置为UEFI-only，或许会导致你装好之后启动文件都不能读到。

  3. 一般来说我应该分几个区？

  分几个区根据你的实际需求来决定，一般一个boot分区，一个跟分区就行。我是个人喜欢再加一个EFI的分区放挂在信息。注意自己的分区规划，以及引导文件的写入保存。

  4. 我安装一些东西总是报错xxx包没有找到？

  找一些靠谱的pacman源，不然可能会出现 error:not found xxx等报错

  5. 为什么别人的archlinux系统那么漂亮，我的那么丑？

  archlinux系统是可以自己改变很多东西的。优化好了很漂亮，可参考官方优化指南或网友的一些博客。





== 2.开源软件xxl_job（分布式任务分发系统）
   xxl_job是一款很不错的分布式任务分发系统，只需要自己在github上下载源码,通过maven命令打包成war包，然后放
到你的tomcat里面就可以跑起来，操作细节作者都写的很清楚。

   参考资料：
   https://github.com/xuxueli/xxl-job/releases

注意点：
1.maven命令打包后，你要的war包并没有产生在你的当前目录下，而是在一个lib文件夹下面，不好找的话可以
直接tree命令一下你的源码文件夹，就会很方便的看到你要的war包。
2.xxl-job分为server端和client端口，和zabbix有点像，不过他们一个是分发任务，一个是监控软件。
3.如果在xxl-job的server端执行自己建立的任务没有成功，建议优先测试双方的端口是否互通，可使用telnet命令。





== 3.git的简单使用

[source, text]
----
   注册自己的github账户，把自己本机的生产的密钥放上去，实现无密码登陆，平时自己写的一些公开的东西也可以放到git上，方便快捷。

   自己研究一下一些简单的命令就能完成一些基本的操作，这里提一下：一定要搞清楚使用git过程中的三个状态：确认提交-提交未确认-暂存
   三个概念：
   工作区：存放git文件的实际目录，自己电脑能看到的目录。
   版本库：工作区有一个隐藏目录.git，是Git的版本库。
   暂存（仅执行add命令）：这个操作就相当于把文件放到了暂存区，conmit就相当于把内容提到了分区

   ----工作区-----          |---版本库----------------------------|
   |            |          |_________________HEAD_______________|
   | file       |  add     |     stage   |          | master  |
   |   file     |----->    |     file    |          |         |
   |     file   |          |     file    |          | file    |
   |            |          |     file    |          | file    |
   |____________|          |_____________|          |_________|



参考文档：https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137396287703354d8c6c01c904c7d9ff056ae23da865a000


☆：写的非常基础、非常细，能适用于一般的应用需要。
----

== 4.python脚本

  参考文档：
  https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000

  学习python中，最好将自己的需求和python相结合来学习，知道自己要实现什么功能，针对性的去学习更快。

  推荐使用工具：PyCharm

  安装第三方模块：pip install  模块名
  批量安装模块：pip install -r requirements.txt（模块的配置文件）

  [he@archlinux ttd-operating-tools $ git checkout -b dev     #切换分支并新建分支
切换到一个新分支 'dev'
[he@archlinux ttd-operating-tools $ git pull origin dev
来自 ssh://scm.mwteck.com:44002/LyuQiang/ttd-operating-tools
 * branch            dev        -> FETCH_HEAD
已经是最新的。
[he@archlinux ttd-operating-tools $ git branch
* dev
  master





== 5.rsync同步

   raync是一个非常好用的数据同步小工具，轻巧便捷，最好的好处是使用过程中支持断点续传，如果是很大的文件不至于浪费很多时间。
且同步的文件可以根据类型，分别用参数命令备份，不至于误删除后不能恢复。

=== 5.1 linux操作系统的使用
   首先两边都得安装rsync服务，一个服务端一个客户端，稍微配置就能使用

.server端 /etc/rsyncd.conf
[source, text]
----
# /etc/rsyncd: configuration file for rsync daemon mode

# See rsyncd.conf man page for more options.

# configuration example:

uid = tom
gid = tom
address = 172.16.2.66
port = 873
use chroot = yes
read only = yes

max connections = 4
pid file = /var/run/rsyncd.pid
# exclude = lost+found/
# transfer logging = yes
# timeout = 900
# ignore nonreadable = yes
# dont compress   = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2
#pid file = /run/rsyncd.pid

motd file = /etc/rsyncd.motd

log file = /var/log/rsync.log
timeout = 300
hosts allow=172.16.1.0/255.255.255.0 172.16.2.0/255.255.255.0 172.16.3.0/255.255.255.0

[mwteck]
path = /root/mwteck
ignore errors
exclude = git/
comment = 100W file

[test]
path = /root/test
list=yes
ignore errors
exclude = paichu/
comment = this is rsyncd.server test
----

client端：
[source, bash]
----
#!/bin/bash

#test目录
/usr/bin/rsync -abvczP  --password-file=/etc/ps  --delete  --backup-dir=/root/172.16.2.66.com/change/test/`date +'%Y-%m-%d-%H-%M-%S'` --log-file=/root/172.16.2.66.com/log/`date +'%Y-%m-%d'`  tom@172.16.2.66::test /root/172.16.2.66.com/source/test

if [ $? -eq 0 ];then
   /bin/echo ">>>>>>>>>>>>>>>>>>>>test dir  rsyn success<<<<<<<<<<<<<<<<"

else
   /bin/echo "test dir rsyn failed"
fi


#守护进程方式：mon目录
/usr/bin/rsync -abvzcP --delete --password-file=/etc/ps  --backup-dir=/root/172.16.2.66.com/change/mon/`date +'%Y-%m-%d-%H-%M-%S'` --log-file=/root/172.16.2.66.com/log/`date +'%Y-%m-%d'` mm@172.16.2.66::mon  /root/172.16.2.66.com/source/mon

if [ $? -eq 0 ];then
   /bin/echo ">>>>>>>>>>>>>>>>>>>>>>>>>daemon_mon rsyn success<<<<<<<<<<<<<<<<<<<<<<<<<<<<"

else
   /bin/echo ">>>>>>>>>>>>>>>>>>>>>>>>>daemon_mon rsyn failed<<<<<<<<<<<<<<<<<<<<<<<<<"
fi
----
上面的配置中是需要的密码验证的，实际应用中根据自己的需要写配置，可以对ip段/用户密码等做限制，也可以弄个ssh的密钥免密码登陆。



=== 5.2 wimdows操作系统
下载软件：http://www.cr173.com/soft/110806.html     （基本就是下一步-下一步的安装）
.server端：（默认位置：C:\Program Files\ICW\rsyncd.conf）
[source, bat]
----
use chroot = false
#strict modes = false
strict modes = true
hosts allow = 172.16.1.0/24 172.16.2.0/24 172.16.3.0/24
max connections = 10
lock file = rsyncd.lock
gid = 0
uid = 0
log file = /cygdrive/d/work/rsyncd.log


# Module definitions
# Remember cygwin naming conventions : c:\work becomes /cygwin/c/work
#
[win7]
path = /cygdrive/d/work/webapps
read only = true
#auth users = mwteck
#secrets file = /cygdrive/d/work/rsync.ps
list = yes
transfer logging = yes
comment = test win7 rsyncd.server
----
注意：配置防火墙开发873端口的出站和入栈，不要轻易关闭防火墙


.client端：
在自己下载的cwrsync文件夹下面：
自建rsync.bat
[source,bat]
----
@echo off

echo.

echo 开始同步数据，请稍等...

echo.



cd D:\cwrsync_5.4.1_x86\cwRsync_5.4.1_x86_Free_client
rsync -abzrvP  --progress  --log-file=/cygdrive/h/wk/error.log --delete --backup-dir=/cygdrive/h/wk/change/ 172.16.1.210::win7 /cygdrive/h/wk/source/webapps/



echo.

echo 数据同步完成

echo.
----

[source,text]
----
注意：
因为windows操作系统之所以可以linux命令时因为虚拟层的转化，但是在虚拟层的转化过程中，因为两者的权限机制不一样，会产生一些东西：比如windows使用cwrsync的过程中，会给备份的文件
自动加一些everyone none的用户权限，然而这个多余的全是应该是被摒弃的。

方法：
1、在cwrsync的服务端做设置，让虚拟层不多做处理，即保持原先的权限不变。
在Windows平台，以客户端形式使用rsync时，同步过来的文件权限是混乱的。仅仅需要继承上级目录权限，作以下设置：
修改server端配置文件：（win7默认路径：C:\Program Files\ICW\etc）
etc/fstab
none /cygdrive cygdrive binary,noacl,posix=0,user 0 0

然后，重新执行rsync命令。

注意：
cygwin1.7以前的rsync，只能这样实现这个需求：（client端的脚本加上命令）

SET CYGWIN=nontsec
rsync ......


2、安装windows使用linux命令的插件Cygwin Terminal，你会发现你client端的备份目录权限组是none,就是这个权限组让你的windows多出来everyone、none用户。(测试第一次成功了，第二次又没成功)
chgrp  Users  workspace
chown user:group  workspace
----

.FAQ（IP以10.10.10.10代替）：

错误一：
----
password file must not be other-accessible
continuing without password file
Password:
rsync客户端路径是否写错，权限设置不对，需要再次输入密码，客户端和服务端的密码文件都应该是600的权限才可以
----
错误二：
----
@ERROR: Unknown module ‘bak’
rsync error: error starting client-server protocol (code 5) at main.c(1522) [receiver= 3.0.3]
服务端server的配置中的[bak]名字和客户端client的10.10.10.10::bak不符
----
错误三：
----
rsync: failed to connect to 10.10.10.10: Connection timed out (110)
rsync error: error in socket IO (code 10) at clientserver.c(124) [receiver=3.0.6]
检查服务端server服务是否正常启动，检查端口防火墙，iptables打开873端口
如果服务端是windows server则在防火墙入站规则中增加873端口
如果服务端是Linux则先检查服务是否启动#ps aux | grep rsync
然后开启873端口#iptables -A INPUT -p tcp --dport 873 -j ACCEPT开启873端口
附：
安装rsync yum install rsync
启动服务/usr/bin/rsync --daemon
启动服务错误failed to create pid file /var/rsyncd.pid: File exists
看看提示服务错误的路径（这个路径不一定就是这个，看自己的报错路径）这里是/var/rsyncd.pid所以
rm -rf /var/rsyncd.pid；再重新启动Rsync服务
此时在看一下ps aux | grep rsync启动成功
----
错误四：
----
@ERROR: access denied to gmz88down from unknown (10.10.10.10)
rsync error: error starting client-server protocol (code 5) at main.c(1503) [receiver=3.0.6]
看看是不是服务端server hosts allow限制了IP，把这里的IP加入到服务端server的hosts allow白名单中，windows rsync不能写多个allow，可以在一个allow中加多个IP，例：hosts allow=10.10.10.10 20.20.20.20
----
错误五：
----
@ERROR: chdir failed
rsync error: error starting client-server protocol (code 5) at main.c(1503) [receiver=3.0.6]
服务端server的目录不存在或者没有权限（要同步的那个文件路径），安装windows rsync时候会创建一个SvcCWRSYNC用户，这个用户对要拷贝的目录没有权限，方法一，将这个用户给权限加入到目录中，方法二，修改这个用户隶属于的组，修改后要在管理中重启服务
----
错误六：
----
rsync error: error starting clie
nt-server protocol (code 5) at main.c(1524) [Receiver= 3.0.7 ]
/etc/rsyncd.conf配置文件内容有错误，检查下配置文件
----
错误七：
----
rsync: ch
own "" failed: Invalid argument (22)
权限无法复制，去掉同步权限的参数即可
----
错误八：
----
@ERROR: auth failed on module bak
rsync error: error starting client-server protocol (code 5) at main.c(1530) [receiver=3.0.6]
密码错误或服务器上是否有bak模块
----
错误九：
----
rsync: connection unexpectedly closed (5 bytes received so far) [sender]
rsync error: error in rsync protocol data stream (code 12) at io.c(600) [sender=3.0.6]
模块read only = no设置为no false
----
错误十：
----
@ERROR: invalid uid nobody
rsync error: error starting client-server protocol (code 5) at main.c(1503) [sender=3.0.6]
设置
uid =0
gid = 0
----
错误十一：
----
rsync: failed to connect to 10.10.10.10: No route to host (113)
rsync error: error in socket IO (code 10) at clientserver.c(124) [receiver=3.0.6]
防火墙原因
----
错误十二：
----
rsync: read error: Connection reset by peer (104)
rsync error: error in rsync protocol data stream (code 12) at io.c(759) [receiver=3.0.6]

/etc/rsyncd.conf配置文件不存在
----
错误十三：
----
rsync: Failed to exec ssh: No such file or directory (2)
rsync error: error in IPC code (code 14) at pipe.c(84) [receiver=3.0.6]
rsync: connection unexpectedly closed (0 bytes received so far) [receiver]
rsync error: error in IPC code (code 14) at io.c(600) [receiver=3.0.6]

需要在客户端安装yum install -y openssh-clients即可
----
错误十四：
----
rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1518) [generator=3.0.9]

服务端没有写权限，修改rsyncd.conf中uid和gid
----


参考博客：
cygwin的一些细节操作：http://oldratlee.com/post/2012-12-22/stunning-cygwin
M19-吴昊 ：https://blog.whsir.com/post-category/linux/rsync （windows server 2008服务器使用（软件包下载） +linux的守护进程使用）




== 6.IDEA的使用

  IDEA是目前比较新的一款java开发集成环境，它里面涵盖了诸多的插件，使我们使用mysql,tomcat等都不用切出去，IDEA+gradle更是一大特色。


参考文档：
jdk+idea+gradle+tomcat环境搭建：
https://www.jianshu.com/p/7fe31ea61dda
servlet+前端：
http://blog.csdn.net/jiuqiyuliang/article/details/36424981/
java+jdbc+mysql:
https://www.cnblogs.com/wuyuegb2312/p/3872607.html

.FAQ:
     1. 为什么我的web项目没有xml配置文件？

     web项目是没有自己生成xml文件的，得自己去手动添加，网友的博客可以帮助你。

     2. 使用gradle我不知道一些包的版本，属于什么包怎那么办？

     需要使用的包都写在buile.gradle里面，不会写的包的格式直接google关键字。

     3. 怎么打包war文件？

     需要打包成war包直接在gradle projects --> 项目名 --> Tasks --> build --> war就成功打包，东西在项目名称下面的lib包里。

这一类的资料网上一大堆，建议优先查看官方文档，详细且版本一致。


== 7.nginx的配置（负载+反代）

[source,text]
----
   nginx的配置一般比较简单，网上文档很多，照着做基本能成功。

   常见的就是基于http协议的负载和反代，还有基于tcp协议的负载和代理。
----

.基于http协议的负载和反代和缓存的部分代码：
ps：nginx.conf主配置文件的http段
[source,bash]
----
   proxy_cache_path /usr/share/nginx/proxy_cache levels=1:1:1 keys_zone=cache:20m max_size=1g;
   # 上述含义为：缓rsync在使用中往往会报错误，综合自己亲身经验，总结几条错误的解决方案（存路径、缓存目录的层级、映射的名称及大小、最大缓存量

   proxy_set_header Host $host;
   proxy_set_header X-Real-IP $remote_addr;  #传递client ip给后端服务器
   proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

server段：
#反代
location / {
    root   /usr/share/nginx/html;
    index  index.html index.htm;
    proxy_pass http://websrvs;
}

add_header X-Via $server_addr;
#动静分离
location ~* \.(jpg|gif|png)$ {
     proxy_pass http://172.16.2.119;
     proxy_cache cache;
     proxy_cache_key $request_uri;
     proxy_cache_valid 200 301 302 1h;
     proxy_cache_valid any 1m;
}

#负载
   upstream websrvs {
   #负载算法：ip_hash/权重(weight)/fair(第三方，谁响应快选谁)/url_hash/
             server 172.16.3.8 weight=2;
             server 172.16.3.19 weight=3;
   }

基于tcp协议的反代：
主配置文件nginx.conf段落：
include /usr/local/nginx-1.6.3/conf/tcp_proxy.conf;

#状态信息模块
location /nginx_status {
        stub_status on;
            access_log  logs/nginx_status.log;
        allow 127.0.0.1;
        deny all;
    }
----

.基于tcp协议的配置：
[source,bash]
----

    include /usr/local/nginx-1.6.3/conf/tcp_proxy.conf;


    tcp {
        upstream mysql {
            server 10.207.238.66:3306;
            server 10.207.238.67:3306;

     #健康检查
            check interval=3000 rise=2 fall=5 timeout=1000;
            #check interval=3000 rise=2 fall=5 timeout=1000
            #check interval=3000 rise=2 fall=5 timeout=1000
            #check_http_send "GET /HTTP/1.0\r\n\r\n";
            #check_http_expect_alive http_2xxhttp_3xx;
        }

        server {
    listen 3307;
            proxy_pass mysql;
        }
    }
----
.注释：
[source,text]
----
    check interval 健康检查，单位是毫秒
    rise 检查几次正常后，将reslserver加入以负载列表中
    fall 检查几次失败后，摘除realserver
    timeout 检查超时时间，单位许毫秒
    具体可查看nginx_tcp_proxy_module-master/README,很详细。
    验证tcp协议的反代成功可以看日志

   参考文档：
   http://blog.51cto.com/1992tao
----


== 8.mysql


- docker上的MySQL配置主从复制
推荐参考文档：https://github.com/Junnplus/blog/issues/1

mysql镜像为5.7，直接pull的
docker pull registry.cn-hangzhou.aliyuncs.com/marmot/mysql-5.7


[source,bash]
----
#创建master容器
[root@archlinux ~]# docker run -h master -p 0.0.0.0:3336:3306 -d -e MYSQL_ROOT_PASSWORD=123456 --name=master registry.cn-hangzhou.aliyuncs.com/marmot/mysql-5.7
0041c855adbdcfe3d7821812a8e0852fde15f54621ad5854eb83134e61121e11
#写个master mysql的主从配置文件
[root@archlinux ~]# cat master.cnf
[mysqld]
server-id = 1
log-bin=mysql-bin
binlog-ignore-db=mysql
# binlog-do-db
# binlog_format=mixed

#把配置文件复制到镜像中
[root@archlinux ~]# docker cp master.cnf master:/etc/mysql/conf.d

#创建slave容器，注意--link参数
[root@archlinux ~]# docker run -h slave -p 0.0.0.0:3337:3306 -d --link master:master -e MYSQL_ROOT_PASSWORD=123456 --name=slave registry.cn-hangzhou.aliyuncs.com/marmot/mysql-5.7
98cbc2b6c736698baea48184f9878e49337fb51e3b7c9f006dbfc64a5de9afad
#写个slave MySQL的配置文件
[root@archlinux ~]# cat slave.cnf
[mysqld]
server-id = 2
# relay_log = relay_bin
# relay-log-index = relay-bin.index
# read-only=1 # 除非有SUPER權限，否則只讀
# super-read-only=1 # SUPER權限也是只讀

#复制配置文件到从库
[root@archlinux ~]# docker cp slave.cnf  slave:/etc/mysql/conf.d
#启动两个容器
[root@archlinux ~]# docker restart master slave
master
slave

#进入容器master库
[root@archlinux ~]# docker exec -it master mysql -uroot -p123456
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.7.21-log MySQL Community Server (GPL)

Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
#创建账号并授权
mysql> grant replication client,replication slave on *.* to 'test'@'%' identified by '123456';
Query OK, 0 rows affected, 1 warning (0.00 sec)

#进入容器slave库
[root@archlinux ~]# docker exec -it slave mysql -uroot -p123456
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.7.21 MySQL Community Server (GPL)

Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
#根据上面主库创建的账号链接到主库
mysql> change master to master_host='master',master_user='test',master_password='123456';
Query OK, 0 rows affected, 2 warnings (0.08 sec)
#启动从库
mysql> start slave;
Query OK, 0 rows affected (0.00 sec)
#查看从库状态
mysql> show slave status\G
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: master
                  Master_User: test
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000001
          Read_Master_Log_Pos: 457
               Relay_Log_File: slave-relay-bin.000002
                Relay_Log_Pos: 670
        Relay_Master_Log_File: mysql-bin.000001
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB:
          Replicate_Ignore_DB:
           Replicate_Do_Table:
       Replicate_Ignore_Table:
      Replicate_Wild_Do_Table:
  Replicate_Wild_Ignore_Table:
                   Last_Errno: 0
                   Last_Error:
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 457
              Relay_Log_Space: 877
              Until_Condition: None
               Until_Log_File:
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File:
           Master_SSL_CA_Path:
              Master_SSL_Cert:
            Master_SSL_Cipher:
               Master_SSL_Key:
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error:
               Last_SQL_Errno: 0
               Last_SQL_Error:
  Replicate_Ignore_Server_Ids:
             Master_Server_Id: 1
                  Master_UUID: 17e741c7-4760-11e8-b32b-0242ac110002
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
           Master_Retry_Count: 86400
                  Master_Bind:
      Last_IO_Error_Timestamp:
     Last_SQL_Error_Timestamp:
               Master_SSL_Crl:
           Master_SSL_Crlpath:
           Retrieved_Gtid_Set:
            Executed_Gtid_Set:
                Auto_Position: 0
         Replicate_Rewrite_DB:
                 Channel_Name:
           Master_TLS_Version:
1 row in set (0.00 sec)

验证：
#docker exec master mysql -uroot -p123456 -e "CREATE DATABASE test"
#docker exec slave mysql -uroot -p123456 -e "SHOW DATABASES"
#主库创建一个库
mysql> create database testdb;
Query OK, 1 row affected (0.01 sec)

#从库查看所有的库
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| testdb             |
+--------------------+
5 rows in set (0.00 sec)



----

.FAQ：
1.执行脚本报错：Index column size too large. The maximum column size is 767 bytes.

解决方法：
[source,bash]
----
set global innodb_file_format = BARRACUDA;
set global innodb_large_prefix = ON;
create table test (........) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;

或者：
innodb_file_format = Barracuda
innodb_file_per_table = 1
innodb_large_prefix
----

2.执行命令报错：mysql> GRANT REPLICATION SLAVE ON *.* TO test@% IDENTIFIED BY  "123456";
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '% IDENTIFIED BY  "123456"' at line 1

解决方法：
[source,bash]
----
[root@archlinux ~]# docker exec -it master mysql -uroot -p123456
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 12
Server version: 8.0.11 MySQL Community Server - GPL

MySQL版本过高所致，这是最开始直接拉去的MySQL镜像，版本为8.0，可能语法有改动，建议用稳定版本
----

== 9.docker

   docker是一款比较轻量级的虚拟化工具，如果单单是把docker部署起来的话自己看几分钟命令，就可以把一个容器跑起
来，或者网上随便抄一个dockerfile就能跑起来，要知道docker跑起来容易让它长期稳定的运行才是我们的追求。

# 启动docker服务
service docker start

# 检索centos镜像
docker search centos

# 下载centos镜像
docker pull centos

docker run -p 172.16.3.38:381:8080 -i -t  -v /niloay/software/:/mnt/software --name ttd_prod_api_gateway   hub.c.163.com/library/tomcat
-p 将docker容器中的8080端口映射到 192.168.0.105的 8080端口 （192.168.0.105是VirtualBox中Centos虚拟机的ip，虚拟机使用桥接模式）

docker run  -h hostname -p  0.0.0.0:5222:22 -p 0.0.0.0:5002:8080 -it  -v /data/web/:/data/web/  --name ttd_arch_api_admin   mwteck/centos6:20180126b1

　　-t  选项让Docker分配一个伪终端（ pseudo-tty）并绑定到容器的标准输入上，

　　-i  则让容器的标准输入保持打开。

　　-v 将本机（虚拟机）的 /niloay/software 挂载到 容器的 /mnt/software目录中

　　--name 容器的名字为appcentos

　　centos 是上面安装的镜像

# 进入容器
docker exec -it dockername /bin/bash
docker exec -it dockername mysql -uroot -p123456

#查看运行中的镜像，加上参数 -a 显示所有容器
[root@archlinux ~]# docker ps
CONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS              PORTS                                          NAMES
2fb47af1c330        mwteck/centos6:20180126b1   "/usr/local/bin/dock…"   18 hours ago        Up 18 hours         0.0.0.0:5522->22/tcp, 0.0.0.0:5500->8080/tcp   test
23039cf93263        mwteck/centos6:20180126b1   "/usr/local/bin/dock…"   2 days ago          Up 2 days           0.0.0.0:5230->22/tcp                           ttd_arch_data_db





参考文档：
docker的中文社区：
http://www.docker.org.cn/


#!/bin/bash

docker run  -h elasticsearch \
  -d \
  --restart always \
  --env LANG=en_US.UTF-8 \
  --env TZ=Asia/Shanghai \
  --mount type=bind,source=/etc/localtime,target=/etc/localtime,readonly \
  -p 5602:5601 -p 9101:9100 -p 9201:9200 -p 9301:9300 -p 9601:9600 \
  -v /root/workspace/elk/data:/data/  \
  --name elk_elasticsearch mwteck/centos6:20180101b1 docker-entrypoint.sh




== 10.ceph(分布式文件系统)

.参考文档：
http://docs.ceph.org.cn/start/intro/
https://github.com/wzyuliyang/testMarkdown/blob/master/rgw%E7%BD%91%E5%85%B3.markdown

- 部署说明：
     主机操作系统 CentOS Linux release 7.4.1708 (Core)
     此次部署将有一个ceph-deploy管理节点和三个ceph存储集群节点,一个客户端.
#集群基础模型～至于后面怎么加功能块看自己～各个节点都可以加
主机名            IP                       扮演角色
node1            172.16.3.38              mon
node2            172.16.3.39              osd
node3            172.16.3.40              osd
client-node      172.16.3.41              测试存储
admin-node      172.16.3.37              admin

- 环境准备：
1.分别修改各个主机名和hosts文件，方便自己理解各个主机的作用和中途的操作。
[source,bash]
----
hostnamectl set-hostname yourname
vim /etc/hosts
----
分别在里面加入
[source,text]
----
172.16.3.38 node1
172.16.3.39 node2
172.16.3.40 node3
172.16.3.41 client-node
172.16.3.42 admin-node
----

分别安装NTP同步时间并同步时间
[source,bash]
----
[root@admin-node ~]# yum -y install ntp ntpdate ntp-doc

[root@admin-node ~]# ntpdate  202.120.2.101
 1 Mar 11:25:25 ntpdate[2740]: adjust time server 202.120.2.101 offset -0.006555 sec
----

本次实验为了方便直接使用的root账户，一般是用自己创建的账户，只需要修改配置文件（/etc/sudoers）获取root权限就行。

允许无密码ssh登陆

创建密钥
[source,bash]
----
[root@admin-node ~]# ssh-keygen
……#全程回车即可
[root@admin-node ~]# cd  ./.ssh/
[root@admin-node .ssh]# ls
authorized_keys  id_rsa  id_rsa.pub  known_hosts
----
分发密钥到各个节点
[source,bash]
----
[root@admin-node .ssh]# ssh-copy-id node1     #第一次直接输入yes然后输入节点密码就行
[root@admin-node .ssh]# ssh-copy-id node2
[root@admin-node .ssh]# ssh-copy-id node3
[root@admin-node .ssh]# ssh-copy-id client-node
----
（推荐做法）修改 ceph-deploy 管理节点上的 ~/.ssh/config 文件，这样 ceph-deploy 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 ceph-deploy 都要指定 --username {username} 。这样做同时也简化了 ssh 和 scp 的用法。
[source,bash]
----
[root@admin-node .ssh]# cat config
Host node1
   Hostname node1
   User root
Host node2
   Hostname node2
   User root
Host node3
   Hostname node3
   User root
Host client-node
   Hostname client-node
   User root
----

关闭防火墙和SELINUX
[source,bash]
----
[root@admin-node ~]# setenforce 0
[root@admin-node ~]# systemctl stop firewalld || systemctl disable firewalld
----

.安装 ceph 部署工具
把 Ceph 仓库添加到 ceph-deploy 管理节点，然后安装 ceph-deploy 。
[source,bash]
----
yum install -y yum-utils && sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ && sudo yum install --nogpgcheck -y epel-release && sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 && sudo rm /etc/yum.repos.d/dl.fedoraproject.org*
----

把软件包源加入软件仓库，用文本编辑器写一个yum库文件
[source,bash]
----
vim /etc/yum.repos.d/ceph.repo
----

把如下内容粘帖进去，用 Ceph 的最新主稳定版名字替换 {ceph-stable-release} （如 firefly ），用你的Linux发行版名字替换 {distro} （如 el6 为 CentOS 6 、 el7 为 CentOS 7 、 rhel6 为 Red Hat 6.5 、 rhel7 为 Red Hat 7 、 fc19 是 Fedora 19 、 fc20 是 Fedora 20 ）。最后保存到 /etc/yum.repos.d/ceph.repo 文件中。

[source,text]
----
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-{ceph-release}/{distro}/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
----
更新软件库并安装 ceph-deploy
[source,bash]
----
yum update && sudo yum install ceph-deploy
----

先在管理节点上创建一个目录，用于保存 ceph-deploy 生成的配置文件和密钥对。

[root@admin-node ~]# mkdir ceph-cluster
[root@admin-node ~]# cd ceph-cluster/

创建集群
在管理节点上，进入刚创建的放置配置文件的目录，用 ceph-deploy 执行如下步骤
----
[root@admin-node ceph-cluster]# ceph-deploy new node1
[root@admin-node ceph-cluster]# ls
ceph.conf  ceph.log  ceph.mon.keyring
[root@admin-node ceph-cluster]# vim ceph.conf
[root@admin-node ceph-cluster]# cat  ceph.conf
[global]
fsid = 97f00feb-29c9-43de-9460-d2bf6a0799b9
mon_initial_members = node1
mon_host = 172.16.3.38
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
filestore_xattr_use_omap = true

osd pool default size = 2

----

在各个节点安装ceph
ceph-deploy install admin-node node1 node2 node3            #建议把各个机子的环境配好，先执行yum -y install ceph-deploy ceph ceph-radosgw

----
[root@admin-node ceph-cluster]# ceph-deploy install admin-node node1 node2 node3 client-node
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy install admin-node node1 node2 node3 client-node
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x1bd4f80>
…………
…………
[client-node][DEBUG ] 12 packages excluded due to repository priority protections
[client-node][DEBUG ] Package 1:ceph-10.2.10-0.el7.x86_64 already installed and latest version
[client-node][DEBUG ] Package 1:ceph-radosgw-10.2.10-0.el7.x86_64 already installed and latest version
[client-node][DEBUG ] Nothing to do
[client-node][INFO  ] Running command: ceph --version
[client-node][DEBUG ] ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)

----






配置初始monitors,并收集所有密钥
ceph-deploy mon create-initial

----
[root@admin-node ceph-cluster]# ceph-deploy mon create-initial
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[ceph_deploy.cli][INFO  ] ceph-deploy options:
…………
…………
[node1][INFO  ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-node1/keyring auth get client.bootstrap-rgw
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmphcrNWu

----

ssh node2
sudo mkdir /var/local/osd0
chown ceph:ceph /var/local/osd0
exit

ssh node3
sudo mkdir /var/local/osd1
chown ceph:ceph /var/local/osd1
exit

准备
[source,bash]
----
ssh node2
mkdir /var/local/osd0
chown ceph:ceph /var/local/osd0
exit

ssh node3
mkdir /var/local/osd1
chown ceph:ceph /var/local/osd1
exit

[root@admin-node ceph-cluster]# ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  block_db                      : None
…………
…………
[node3][WARNIN] command: Running command: /usr/bin/chown -R ceph:ceph /var/local/osd1/magic.999.tmp
[node3][INFO  ] checking OSD status...
[node3][DEBUG ] find the location of an executable
[node3][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node3 is now ready for osd use.

----


激活
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
…………
…………
[node3][WARNIN] command_check_call: Running command: /usr/bin/systemctl start ceph-osd@1
[node3][INFO  ] checking OSD status...
[node3][DEBUG ] find the location of an executable
[node3][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json
[node3][INFO  ] Running command: systemctl enable ceph.target
----


用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了。
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy admin admin-node node1 node2 node3
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin admin-node node1 node2 node3
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x2195908>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['admin-node', 'node1', 'node2', 'node3']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f3cd488c8c0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to admin-node
[admin-node][DEBUG ] connected to host: admin-node
[admin-node][DEBUG ] detect platform information from remote host
[admin-node][DEBUG ] detect machine type
[admin-node][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node1
[node1][DEBUG ] connected to host: node1
[node1][DEBUG ] detect platform information from remote host
[node1][DEBUG ] detect machine type
[node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node2
[node2][DEBUG ] connected to host: node2
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node3
[node3][DEBUG ] connected to host: node3
[node3][DEBUG ] detect platform information from remote host
[node3][DEBUG ] detect machine type
[node3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf

----

确保你对 ceph.client.admin.keyring 有正确的操作权限。
[source,bash]
----
[root@admin-node ceph-cluster]# chmod +r /etc/ceph/ceph.client.admin.keyring
……
[root@node1 ~]# chmod +r /etc/ceph/ceph.client.admin.keyring
[root@node1 ~]# ls /etc/ceph/ceph.client.admin.keyring -l
-rw-r--r--. 1 root root 129 Mar  2 10:16 /etc/ceph/ceph.client.admin.keyring
----

查看集群的状态
[source,bash]
----
[root@admin-node ceph-cluster]# ceph -s
    cluster 25319ec9-a963-4759-9fe7-9f8f37b64c90
     health HEALTH_OK
     monmap e1: 1 mons at {node1=172.16.3.38:6789/0}
            election epoch 3, quorum 0 node1
     osdmap e10: 2 osds: 2 up, 2 in
            flags sortbitwise,require_jewel_osds
      pgmap v24: 64 pgs, 1 pools, 0 bytes data, 0 objects
            13320 MB used, 43980 MB / 57300 MB avail
                  64 active+clean
[root@admin-node ceph-cluster]# ceph health
HEALTH_OK
----

- 拓展集群
[source,bash]
----
[root@admin-node ceph-cluster]# ssh node1
Last login: Fri Mar  2 08:27:11 2018 from 172.16.3.30
[root@node1 ~]# mkdir /var/local/osd2
[root@node1 ~]# chown ceph:ceph /var/local/osd2
[root@node1 ~]# exit
logout
Connection to node1 closed.
[root@admin-node ceph-cluster]# ceph-deploy osd prepare node1:/var/local/osd2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare node1:/var/local/osd2
[node1][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node1 is now ready for osd use.
[root@admin-node ceph-cluster]# ceph-deploy osd activate node1:/var/local/osd2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd activate node1:/var/local/osd2

----



[source,bash]
----
[root@admin-node ceph-cluster]# ceph -s
    cluster 25319ec9-a963-4759-9fe7-9f8f37b64c90
     health HEALTH_OK
     monmap e1: 1 mons at {node1=172.16.3.38:6789/0}
            election epoch 3, quorum 0 node1
     osdmap e15: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v59: 64 pgs, 1 pools, 0 bytes data, 0 objects
            19985 MB used, 65964 MB / 85950 MB avail
                  64 active+clean


----


添加元数据服务器，暂时官方不提供多个元数据服务器技术支持
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy mds create node1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mds create node1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x1150200>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function mds at 0x10ef5f0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  mds                           : [('node1', 'node1')]
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.mds][DEBUG ] Deploying mds, cluster ceph hosts node1:node1
[node1][DEBUG ] connected to host: node1
[node1][DEBUG ] detect platform information from remote host
[node1][DEBUG ] detect machine type
[ceph_deploy.mds][INFO  ] Distro info: CentOS Linux 7.4.1708 Core
[ceph_deploy.mds][DEBUG ] remote host will use systemd
[ceph_deploy.mds][DEBUG ] deploying mds bootstrap to node1
[node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node1][DEBUG ] create path if it doesn't exist
[node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-mds --keyring /var/lib/ceph/bootstrap-mds/ceph.keyring auth get-or-create mds.node1 osd allow rwx mds allow mon allow profile mds -o /var/lib/ceph/mds/ceph-node1/keyring
[node1][INFO  ] Running command: systemctl enable ceph-mds@node1
[node1][WARNIN] Created symlink from /etc/systemd/system/ceph-mds.target.wants/ceph-mds@node1.service to /usr/lib/systemd/system/ceph-mds@.service.
[node1][INFO  ] Running command: systemctl start ceph-mds@node1
[node1][INFO  ] Running command: systemctl enable ceph.target

----


添加对象网关
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy rgw create node1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy rgw create node1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  rgw                           : [('node1', 'rgw.node1')]
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x18f5170>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function rgw at 0x18396e0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts node1:rgw.node1
[node1][DEBUG ] connected to host: node1
[node1][DEBUG ] detect platform information from remote host
[node1][DEBUG ] detect machine type
[ceph_deploy.rgw][INFO  ] Distro info: CentOS Linux 7.4.1708 Core
[ceph_deploy.rgw][DEBUG ] remote host will use systemd
[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to node1
[node1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node1][DEBUG ] create path recursively if it doesn't exist
[node1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.node1 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.node1/keyring
[node1][INFO  ] Running command: systemctl enable ceph-radosgw@rgw.node1
[node1][WARNIN] Created symlink from /etc/systemd/system/ceph-radosgw.target.wants/ceph-radosgw@rgw.node1.service to /usr/lib/systemd/system/ceph-radosgw@.service.
[node1][INFO  ] Running command: systemctl start ceph-radosgw@rgw.node1
[node1][INFO  ] Running command: systemctl enable ceph.target
[ceph_deploy.rgw][INFO  ] The Ceph Object Gateway (RGW) is now running on host node1 and default port 7480

----

RGW 例程默认会监听 7480 端口，可以更改该节点 ceph.conf 内与 RGW 相关的配置，如下
[source,bash]
----
[root@admin-node ceph-cluster]# cat  ceph.conf
[global]
fsid = 25319ec9-a963-4759-9fe7-9f8f37b64c90
mon_initial_members = node1
mon_host = 172.16.3.38
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
osd pool default size = 2


[client]
rgw frontends = civetweb port=80
----



[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy mon add node2 node3
usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                   [--overwrite-conf] [--ceph-conf CEPH_CONF]
                   COMMAND ...
ceph-deploy: error: unrecognized arguments: node3

----


块设备的使用
[source,bash]
----
[root@admin-node ceph-cluster]# ceph-deploy install client-node
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy install client-node
[ceph_deploy.cli][INFO  ] ceph-deploy options:
…………
…………
[root@admin-node ceph-cluster]# ceph-deploy admin client-node
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client-node
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x1a0b908>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['client-node']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fef43af68c0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client-node
[client-node][DEBUG ] connected to host: client-node
[client-node][DEBUG ] detect platform information from remote host
[client-node][DEBUG ] detect machine type
[client-node][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[root@admin-node ceph-cluster]# ssh client-node
Last login: Fri Mar  2 08:28:07 2018 from 172.16.3.30
[root@client-node ~]# chmod +r /etc/ceph/ceph.client.admin.keyring
[root@client-node ~]# rbd create rbd/kuai-db2 --size 10G --image-format 2 --image-feature  layering
[root@client-node ~]# rbd ls
kuai-db
kuai-db2
[root@client-node ~]# rbd map rbd/kuai-db2
/dev/rbd1
[root@client-node ~]# mkdir /mnt/Block-device
[root@client-node ~]# mkfs.ext4 -m0 /dev/rbd1
mke2fs 1.42.9 (28-Dec-2013)
Discarding device blocks: done
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=1024 blocks, Stripe width=1024 blocks
655360 inodes, 2621440 blocks
0 blocks (0.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=2151677952
80 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks:
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done
Writing inode tables: done
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done

[root@client-node ~]# lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   30G  0 disk
├─sda1            8:1    0    1G  0 part /boot
└─sda2            8:2    0   29G  0 part
  ├─centos-root 253:0    0   28G  0 lvm  /
  └─centos-swap 253:1    0    1G  0 lvm  [SWAP]
sr0              11:0    1 1024M  0 rom
rbd0            252:0    0   10G  0 disk
rbd1            252:16   0   10G  0 disk /mnt/Block-device

----


文件系统：
----
[root@client-node Block-device]# ceph osd pool create cephfs_data 256 256
pool 'cephfs_data' created
[root@client-node Block-device]# ceph osd pool create cephfs_metadata 256 256
pool 'cephfs_metadata' created
[root@client-node Block-device]# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 9 and data pool 8
[root@client-node Block-device]# ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
[root@client-node Block-device]# ceph mds stat
e5: 1/1/1 up {0=node1=up:active}


[root@client-node ~]# ceph-fuse -m node1:6789 /mnt/cephFS
ceph-fuse[1763]: starting ceph client
2018-03-02 13:53:00.734996 7fc6c36c2ec0 -1 init, newargv = 0x562af0a827e0 newargc=11
ceph-fuse[1763]: starting fuse

#等一会儿就好了？ lsblk看不到是因为这个命令是只看块设备的ge  但是df -h 可以


----

对象存储

----
[root@admin-node ceph-cluster]# ceph-deploy install --rgw node3   #安装对象网关
[root@admin-node ceph-cluster]# ceph-deploy admin node3    #设置节点管理权限
[root@admin-node ceph-cluster]# ceph-deploy rgw create node3 #新建网关实例

为了使用 REST 接口，首先需要为S3接口创建一个初始 Ceph 对象网关用户。然后，为 Swift 接口创建一个子用户。然后你需要验证创建的用户是否能够访问网关。

为 S3 访问创建 RADOSGW 用户
一个``radosgw`` 用户需要被新建并被分配权限。命令 man radosgw-admin 会提供该命令的额外信息。
[root@node3 ~]# sudo radosgw-admin user create --uid="testuser" --display-name="First User"

命令的输出跟下面的类似:
[source,text]
----
{
        "user_id": "testuser",
        "display_name": "First User",
        "email": "",
        "suspended": 0,
        "max_buckets": 1000,
        "auid": 0,
        "subusers": [],
        "keys": [{
                "user": "testuser",
                "access_key": "I0PJDPCIYZ665MW88W9R",
                "secret_key": "dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA"
        }],
        "swift_keys": [],
        "caps": [],
        "op_mask": "read, write, delete",
        "default_placement": "",
        "placement_tags": [],
        "bucket_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "user_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "temp_url_keys": []
}

Note 其中 keys->access_key 和``keys->secret_key`` 的值在访问的时候需要用来做验证
----
新建一个 SWIFT 用户
如果你想要使用这种方式访问集群，你需要新建一个 Swift 子用户。创建 Swift 用户包括两个步骤。第一步是创建用户。第二步是创建 secret key。
[root@node3 ~]# adosgw-admin subuser create --uid=testuser --subuser=testuser:swift --access=full
输出类似下面这样:
[source,text]
----
{
        "user_id": "testuser",
        "display_name": "First User",
        "email": "",
        "suspended": 0,
        "max_buckets": 1000,
        "auid": 0,
        "subusers": [{
                "id": "testuser:swift",
                "permissions": "full-control"
        }],
        "keys": [{
                "user": "testuser:swift",
                "access_key": "3Y1LNW4Q6X0Y53A52DET",
                "secret_key": ""
        }, {
                "user": "testuser",
                "access_key": "I0PJDPCIYZ665MW88W9R",
                "secret_key": "dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA"
        }],
        "swift_keys": [],
        "caps": [],
        "op_mask": "read, write, delete",
        "default_placement": "",
        "placement_tags": [],
        "bucket_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "user_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "temp_url_keys": []
 }
----
[root@node3 ~]#sudo radosgw-admin key create --subuser=testuser:swift --key-type=swift --gen-secret
[source,text]
----
{
        "user_id": "testuser",
        "display_name": "First User",
        "email": "",
        "suspended": 0,
        "max_buckets": 1000,
        "auid": 0,
        "subusers": [{
                "id": "testuser:swift",
                "permissions": "full-control"
        }],
        "keys": [{
                "user": "testuser:swift",
                "access_key": "3Y1LNW4Q6X0Y53A52DET",
                "secret_key": ""
        }, {
                "user": "testuser",
                "access_key": "I0PJDPCIYZ665MW88W9R",
                "secret_key": "dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA"
        }],
        "swift_keys": [{
                "user": "testuser:swift",
                "secret_key": "244+fz2gSqoHwR3lYtSbIyomyPHf3i7rgSJrF\/IA"
        }],
        "caps": [],
        "op_mask": "read, write, delete",
        "default_placement": "",
        "placement_tags": [],
        "bucket_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "user_quota": {
                "enabled": false,
                "max_size_kb": -1,
                "max_objects": -1
        },
        "temp_url_keys": []
}
----


----
查看本机某用户的信息
[root@node3 ~]# ceph -s  #看健康状况
[root@node3 ~]# ceph df  #看存储量大小
[root@node3 ~]# radosgw-admin user info --uid=testuser    #查看网关用户信息
{
    "user_id": "testuser",
    "display_name": "First User",
    "email": "",
    "suspended": 0,
    "max_buckets": 1000,
    "auid": 0,
    "subusers": [
        {
            "id": "testuser:swift",
            "permissions": "full-control"
        }
    ],
    "keys": [
        {
            "user": "testuser",
            "access_key": "GWHSA3ZECQ9COFG6UK9J",
            "secret_key": "fnartFbney3zsDQOxABk16PdAOtVP1acnFoOgU8P"
        }
    ],
    "swift_keys": [
        {
            "user": "testuser:swift",
            "secret_key": "c8CSYQclJPojcXs8gD9lgjrtJfBTpbbElrK5dYJg"
        }
    ],
    "caps": [],
    "op_mask": "read, write, delete",
    "default_placement": "",
    "placement_tags": [],
    "bucket_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    },
    "user_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    },
    "temp_url_keys": []
}

----

测试 S3 访问
为了验证 S3 访问，你需要编写并运行一个 Python 测试脚本。S3 访问测试脚本将连接 radosgw, 新建一个新的 bucket 并列出所有的 buckets。 aws_access_key_id 和 aws_secret_access_key 的值来自于命令``radosgw_admin`` 的返回值 access_key 和 secret_key 。
[root@node3 ~]# yum install python-boto

[root@node3 ~]# vi s3test.py
import boto
import boto.s3.connection
#模板
access_key = 'I0PJDPCIYZ665MW88W9R'
secret_key = 'dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA'
conn = boto.connect_s3(
        aws_access_key_id = access_key,
        aws_secret_access_key = secret_key,
        host = '{hostname}', port = {port},
        is_secure=False, calling_format = boto.s3.connection.OrdinaryCallingFormat(),
        )

bucket = conn.create_bucket('my-new-bucket')
    for bucket in conn.get_all_buckets():
            print "{name}".format(
                    name = bucket.name,
                    created = bucket.creation_date,
 )
#实际
[root@node3 ~]# cat s3test.py
import boto
import boto.s3.connection

access_key = 'GWHSA3ZECQ9COFG6UK9J'
secret_key = 'fnartFbney3zsDQOxABk16PdAOtVP1acnFoOgU8P'
conn = boto.connect_s3(
        aws_access_key_id = access_key,
        aws_secret_access_key = secret_key,
        host = '172.16.3.40', port =7480,
        is_secure=False, calling_format = boto.s3.connection.OrdinaryCallingFormat(),
        )

bucket = conn.create_bucket('my-01-bucket')
for bucket in conn.get_all_buckets():
            print "{name}".format(name = bucket.name,created = bucket.creation_date,)

#注意修改host和端口

[root@node3 ~]# python s3test.py
my-new-bucket                    #即为验证成功

……
……
[root@node3 ~]# radosgw-admin bucket list  #查看所有的bucket
[
    "my-new-bucket",
    "my-01-bucket"
]


#管理并操作bucket
[root@node3 ~]# vi ~/.s3cfg
//修改 access_key  host_base  host_bucket  secret_key

access_key = GWHSA3ZECQ9COFG6UK9J
secret_key = fnartFbney3zsDQOxABk16PdAOtVP1acnFoOgU8P
host_base = node3          #如果这里你的端口为80可以不管，如果为7480（默认）要写成：host_base = node3:7480
host_bucket = %(*)s.node3  #这儿这个括号里面的要么为自己的bucket名称要么为×号（×号为所有的bucket）

#显示全部bucket名称
[root@node3 ~]# s3cmd ls
2018-03-21 09:14  s3://my-01-bucket
2018-03-22 16:24  s3://testdb-bucket

#创建 bucket，且 bucket 名称是唯一的，不能重复。
[root@node3 ~]# s3cmd mb s3://my-bucket-name

#删除空 bucket
[root@node3 ~]# s3cmd rb s3://my-bucket-name

#列举 Bucket 中的内容
[root@node3 ~]# s3cmd ls s3://my-bucket-name

#上传 file.txt 到某个 bucket，
[root@node3 ~]# s3cmd put file.txt s3://my-bucket-name/file.txt

#上传并将权限设置为所有人可读
[root@node3 ~]# s3cmd put --acl-public file.txt s3://my-bucket-name/file.txt

eg:
[root@node3 ~]# s3cmd put --acl-public anaconda-ks.cfg s3://testdb-bucket/
upload: 'anaconda-ks.cfg' -> 's3://testdb-bucket/anaconda-ks.cfg'  [1 of 1]
 1218 of 1218   100% in    0s    16.56 kB/s  done
Public URL of the object is: http://node3:7480/testdb-bucket/anaconda-ks.cfg
上传的文件是可以直接通过网络访问的

#批量上传文件
[root@node3 ~]# s3cmd put ./* s3://my-bucket-name/

#下载文件
[root@node3 ~]# s3cmd get s3://my-bucket-name/file.txt file.txt

#批量下载
[root@node3 ~]# s3cmd get s3://my-bucket-name/* ./

#删除文件
[root@node3 ~]# s3cmd del s3://my-bucket-name/file.txt

#来获得对应的bucket所占用的空间大小
[root@node3 ~]# s3cmd du -H s3://my-bucket-name





















----


.FAQ：
1. 为什么我的源总是出问题？
  因为一开始源的配置就没弄好，可以参考官方文档里面的配置ceph.repo源文件。
  http://docs.ceph.org.cn/start/quick-start-preflight/#ceph




2.RuntimeError: NoSectionError: No section: 'ceph' 【没有找到ceph】
[ceph_node01][DEBUG ] ceph-release-1-1.el7                  ########################################
[ceph_node01][WARNIN] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[ceph_deploy][ERROR ] RuntimeError: NoSectionError: No section: 'ceph'
…………
[ceph_node01][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: yum -y install ceph ceph-radosgw

问题所在：可能是因为网络原因，导致admin端没成功给node1安装ceph ceph-radosgw导致的,建议直接自己手动在node1安装

[ceph@ceph_node01 my-cluster]$ sudo yum -y install ceph ceph-radosgw
Loaded plugins: fastestmirror, priorities
Repository base is listed more than once in the configuration





3、安装报错，缺包
--> Finished Dependency Resolution
Error: Package: 1:ceph-common-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libbabeltrace-ctf.so.1()(64bit)
Error: Package: 1:ceph-mon-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libleveldb.so.1()(64bit)
Error: Package: 1:librbd1-10.2.10-0.el7.x86_64 (Ceph)
           Requires: liblttng-ust.so.0()(64bit)
Error: Package: 1:ceph-common-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libbabeltrace.so.1()(64bit)
Error: Package: 1:librgw2-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libfcgi.so.0()(64bit)
Error: Package: 1:ceph-base-10.2.10-0.el7.x86_64 (Ceph)
           Requires: liblttng-ust.so.0()(64bit)
Error: Package: 1:ceph-osd-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libleveldb.so.1()(64bit)
Error: Package: 1:librados2-10.2.10-0.el7.x86_64 (Ceph)
           Requires: liblttng-ust.so.0()(64bit)
Error: Package: 1:ceph-radosgw-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libfcgi.so.0()(64bit)
Error: Package: 1:ceph-radosgw-10.2.10-0.el7.x86_64 (Ceph)
           Requires: libfcgi.so.0()(64bit)
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
没有yum的epel-release源导致的，缺libfcgi.so的包
建议手动安装
yum -y install epel-release



4、源的地址没有找到
[admin-node][INFO  ] Running command: rpm --import https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc
[admin-node][INFO  ] Running command: rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[admin-node][DEBUG ] Retrieving http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[admin-node][WARNIN] error: open of <html> failed: No such file or directory
[admin-node][WARNIN] error: open of <head><title>Index failed: No such file or directory
[admin-node][WARNIN] error: open of of failed: No such file or directory
…………
[admin-node][ERROR ] RuntimeError: command returned non-zero exit status: 32
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm

源有问题建议换一个 例如：http://ceph.com/rpm-hammer/el7/noarch




5、个别节点down了
[root@admin-node ceph-cluster]# ceph osd tree
ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.08189 root default
-2 0.02730     host node2
 0 0.02730         osd.0       up  1.00000          1.00000
-3 0.02730     host node3
 1 0.02730         osd.1     down        0          1.00000
-4 0.02730     host node1
 2 0.02730         osd.2       up  1.00000        node02
[ceph_node02][DEBUG ] detect platform information from remote hosts  1.00000
-5       0     host ttd

 这个问题在于osd.1节点down掉了，我们重新启动起来就好

 [root@admin-node ceph-cluster]# ceph-deploy osd activate  node3:/var/local/osd1




6、[ceph@ceph_node01 my-cluster]$ sudo ceph-deploy mon create-initial
[sudo] password for ceph:
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy mon create-initial
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x1198908>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x118e5f0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  keyrings                      : None
[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts ceph_node02
[ceph_deploy.mon][DEBUG ] detecting platform for host ceph_node02 ...
[ceph_node02][DEBUG ] connected to host: ceph_node02
[ceph_node02][DEBUG ] detect platform information from remote hosts
[ceph_node02][DEBUG ] detect machine type
[ceph_node02][DEBUG ] find the location of an executable
[ceph_deploy.mon][ERROR ] ceph needs to be installed in remote host: ceph_node02
[ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

“这个做测试的时候忘了怎么解决的。。”



7、随便执行什么ceph的命令就一直弹这个报错
[root@node3 ~]# ceph -s
2018-03-21 11:52:18.676487 7f2f8c7e2700  0 -- :/785802200 >> 172.16.3.38:6789/0 pipe(0x7f2f88064170 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7f2f8805c7d0).fault
2018-03-21 11:52:21.677960 7f2f8c6e1700  0 -- :/785802200 >> 172.16.3.38:6789/0 pipe(0x7f2f7c000c80 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7f2f7c001f90).fault
^C2018-03-21 11:52:24.678869 7f2f8c7e2700  0 -- :/785802200 >> 172.16.3.38:6789/0 pipe(0x7f2f7c0052b0 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7f2f7c006570).fault
问题所在：防火墙的问题，关了就OK
解决：各个节点执行
[root@admin-node ~]# iptables -F
[root@admin-node ~]# systemctl stop firewalld
[root@admin-node ~]# systemctl enable firewalld



8、链接拒绝
[root@node3 ~]# python s3test1.py
Traceback (most recent call last):
  File "s3test1.py", line 12, in <module>
    bucket = conn.create_bucket('my-new-bucket')
  File "/usr/lib/python2.7/site-packages/boto/s3/connection.py", line 615, in create_bucket
    response = self.make_request('PUT', bucket_name, headers=headers,data=data)
  File "/usr/lib/python2.7/site-packages/boto/s3/connection.py", line 667, in make_request
    retry_handler=retry_handler
  File "/usr/lib/python2.7/site-packages/boto/connection.py", line 1071, in make_request
    retry_handler=retry_handler)
  File "/usr/lib/python2.7/site-packages/boto/connection.py", line 1030, in _mexe
    raise ex
socket.error: [Errno 111] Connection refused
 问题所在：7480端口没启动
[root@node3 ~]# ss -ntl
State      Recv-Q Send-Q                                  Local Address:Port                                                 Peer Address:Port
LISTEN     0      128                                                 *:6800                                                            *:*
LISTEN     0      128                                       172.16.3.40:6804                                                            *:*
LISTEN     0      128                                       172.16.3.40:6805                                                            *:*
LISTEN     0      128                                       172.16.3.40:6806                                                            *:*
LISTEN     0      128                                                 *:22                                                              *:*
LISTEN     0      100                                         127.0.0.1:25                                                              *:*
LISTEN     0      100                                                :::8080                                                           :::*
LISTEN     0      128                                                :::22                                                             :::*
LISTEN     0      100                                               ::1:25                                                             :::*
LISTEN     0      1                                    ::ffff:127.0.0.1:8005                                                           :::*
LISTEN     0      100                                                :::8009

解决方法：重启服务
[root@node3 ~]# systemctl restart ceph-radosgw.target


9、 健康警告
[root@node1 ~]# ceph -s
    cluster 25319ec9-a963-4759-9fe7-9f8f37b64c90
     health HEALTH_WARN
            too many PGs per OSD (437 > max 300)
     monmap e1: 1 mons at {node1=172.16.3.38:6789/0}
            election epoch 9, quorum 0 node1
      fsmap e35: 1/1/1 up {0=node1=up:active}
     osdmap e322: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v87195: 656 pgs, 13 pools, 136 MB data, 264 objects
            30731 MB used, 55218 MB / 85950 MB avail
                 656 active+clean
                                               :::*
问题原因是集群osd 数量较少，在我的测试过程中，由于搭建rgw网关、和OpenStack集成等，创建了大量的pool，每个pool要占用一些pg ，ceph集群默认每块磁盘都有默认值，好像每个osd 为300个pgs，不过这个默认值是可以调整的，但调整得过大或者过小都会对集群的性能产生一定影响。因为我们这个是测试环境，只要能消除掉报错即可。查询当前每个osd下最大的pg报警值：

$ ceph --show-config  | grep mon_pg_warn_max_per_osd
p.s:如果没写默认是300,我们修改了就ok
在mon节点的ceph.conf（/etc/ceph/ceph.conf）配置文件中添加:

$ vi /etc/ceph/ceph.conf
[global]
.......
mon_pg_warn_max_per_osd = 1000
重启monitor服务：

$ systemctl restart ceph-mon.target


10、[root@node3 ~]# s3cmd ls
ERROR: S3 error: 403 (RequestTimeTooSkewed): The difference between the request time and the current time is too large.
问题所在：时间不对



11、[root@node3 ~]# python news3.py
Traceback (most recent call last):
  File "news3.py", line 13, in <module>
    bucket = conn.create_bucket('my-new-bucket')
  File "/usr/lib/python2.7/site-packages/boto/s3/connection.py", line 619, in create_bucket
    response.status, response.reason, body)
boto.exception.S3CreateError: S3CreateError: 409 Conflict
<?xml version="1.0" encoding="UTF-8"?><Error><Code>BucketAlreadyExists</Code><BucketName>my-new-bucket</BucketName><RequestId>tx000000000000000000030-005ab3e75a-1217d-default</RequestId><HostId>1217d-default-default</HostId></Error>
问题所在：BucketAlreadyExists 在ceph的使用过程中bucket的名称必须是唯一的，这里的报错就是因为名称已经存在了。
解决方法：改一下测试脚本里面的mucket名称即可。


12、[root@node3 ~]# s3cmd ls

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    An unexpected error has occurred.
  Please try reproducing the error using
  the latest s3cmd code from the git master
  branch found at:
    https://github.com/s3tools/s3cmd
  and have a look at the known issues list:
    https://github.com/s3tools/s3cmd/wiki/Common-known-issues-and-their-solutions
  If the error persists, please report the
  following lines (removing any private
  info as necessary) to:
   s3tools-bugs@lists.sourceforge.net


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Invoked as: /usr/bin/s3cmd ls
Problem: error: [Errno 111] Connection refused
S3cmd:   1.6.1
python:   2.7.5 (default, Nov 20 2015, 02:00:19)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]
environment LANG=zh_CN.UTF-8

问题所在： 请注意 报错 ：Problem: error: [Errno 111] Connection refused 。这种链接被拒绝一般是注意两个点：地址+端口。通过打印debug信息发现我们/root/.s3cfg里面寻找的端口是80，而我的端口是7480（这个端口号看你自己的端口是多少，默认为7480，一般根据官方文档做的可能为80）
解决方法：
修改/root/.s3cfg文件里面的配置为：host_base = node3:7480




- ELK

【source,text】
----
日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。

通常，日志被分散的储存不同的设备上。如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理，例如：开源的syslog，将所有服务器上的日志收集汇总。

集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心。

开源实时日志分析ELK平台能够完美的解决我们上述的问题，ELK由ElasticSearch、Logstash和Kiabana三个开源工具组成。官方网站：https://www.elastic.co/products

----


Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。

Logstash是一个完全开源的工具，他可以对你的日志进行收集、过滤，并将其存储供以后使用（如，搜索）。

Kibana 也是一个开源和免费的工具，它Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。





ELK官网：https://www.elastic.co/
ELK官网文档：https://www.elastic.co/guide/index.html
ELK中文指南：https://kibana.logstash.es/content/
Elasticsearch 管理：https://www.elastic.co/guide/en/elasticsearch/guide/current/administration.html

- ELK平台搭建

ELK下载：https://www.elastic.co/downloads/

[root@node1 ~]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.3.tar.gz
[root@node1 ~]# ls
elasticsearch-6.2.3.tar.gz
[root@node1 ~]# tar -zxvf elasticsearch-6.2.3.tar.gz
[root@node1 ~]# mv elasticsearch-6.2.3 elasticsearchrr
[root@node1 ~]# cd elasticsearch
[root@node1 elasticsearch]# ls
bin  config  lib  LICENSE.txt  logs  modules  NOTICE.txt  plugins  README.textile
[root@node1 elasticsearch]# vim config/elasticsearch.yml               //修改配置文件
[root@node1 elasticsearch]# bin/elasticsearch -d


[root@node2 ~]# wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.3.tar.gz
[root@node2 ~]# ls
anaconda-ks.cfg  logstash-6.2.3  logstash-6.2.3.tar.gz
[root@node2 ~]# mv logstash-6.2.3 logstash
[root@node2 ~]# cd logstashc && ls
-bash: cd: logstashc: No such file or directory
[root@node2 ~]# cd logstash && ls
bin  config  CONTRIBUTORS  data  Gemfile  Gemfile.lock  lib  LICENSE  logstash-core  logstash-core-plugin-api  modules  NOTICE.TXT  tools  vendor




[root@node1 ~]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.3.rpm
[root@node1 ~]# rpm -ivh elasticsearch-6.2.3.rpm
warning: elasticsearch-6.2.3.rpm: Header V4 RSA/SHA512 Signature, key ID d88e42b4: NOKEY
Preparing...                          ################################# [100%]
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Updating / installing...
   1:elasticsearch-0:6.2.3-1          ################################# [100%]
### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing
 sudo systemctl start elasticsearch.service
[root@node1 ~]# systemctl daemon-reload
[root@node1 ~]# systemctl enable elasticsearch.service
Created symlink from /etc/systemd/system/multi-user.target.wants/elasticsearch.service to /usr/lib/systemd/system/elasticsearch.service.
[root@node1 ~]# systemctl start elasticsearch.service


curl -XGET node1:9200/_cluster/health?pretty


curl 'localhost:9200/_cat/indices?v'


启动方式：
e:./bin/elasticsearch
l:[root@node3 logstash]# logstash -f  ./etc/elk.conf
k:[root@node3 elasticsearch-head]# npm run start
[root@node3 kibana]# kibana





ERROR: bootstrap checks failed
max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536]
max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
解决方案：
修改/etc/sysctl.conf 文件，添加 “vm.max_map_count”设置
vm.max_map_count=655360
并执行：sysctl -p
vi /etc/security/limits.conf

添加如下内容:
* soft nofile 65536
* hard nofile 131072
* soft nproc 2048
* hard nproc 4096



[2018-04-17T15:15:24,930][INFO ][o.e.d.z.ZenDiscovery     ] [node-1] failed to send join request to master [{node-3}{W8tsblvrQrGjCdIXe7pPSA}{x3J0JAhpTBuIiAdUxizDxg}{172.16.3.7}{172.16.3.7:9300}], reason [RemoteTransportException[[node-3][172.16.3.7:9300][internal:discovery/zen/join]]; nested: ConnectTransportException[[node-1][172.16.3.38:9300] connect_exception]; nested: IOException[No route to host: 172.16.3.38/172.16.3.38:9300]; nested: IOException[No route to host]; ]

防火墙的原因，关了就行.注意是都要关闭！
[root@node3 ~]# iptables -F
[root@node3 ~]# systemctl disable firewalld
[root@node3 ~]# systemctl stop firewalld



FQ:elasticsearch主页面看到的索引节如果和自己设定的索引节点不一致，怎么刷新都没用
可以尝试重启logstash，我自己是每重启一次多显示出来一个节点。（理论上是一次都加载出来的）
